{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](http://yann.lecun.com/exdb/mnist/) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "95.3% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade ipywidgets\n",
    "from ipywidgets import FloatProgress\n",
    "# Define transforms\n",
    "## YOUR CODE HERE ##\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "data_transforms = transforms.Compose([ \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5),(0.5))\n",
    "])\n",
    "\n",
    "# Create training set and define training dataloader\n",
    "## YOUR CODE HERE ##\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=64)\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "## YOUR CODE HERE ##\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your preprocessing\n",
    "\n",
    "In your own words, why did you choose the transforms you chose? If you didn't use any preprocessing steps, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "In order to properly prepare data for training it is necessary to use preprocessing steps, such as data reshaping, to match the size of the images in the dataset (not needed in this case, as MNIST dataset is already preprocessed), normalization to help improve performance, and flattening to help model interpret images (within the model in this notebook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  torch.Size([60000, 28, 28])\n",
      "Training data targets size:  torch.Size([60000])\n",
      "Shape:  torch.Size([1, 28, 28])\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore data\n",
    "## YOUR CODE HERE ##\n",
    "training_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=5)\n",
    "show5(train_dataloader)\n",
    "\n",
    "# dataset size\n",
    "image, label = training_data[0]\n",
    "print('Training data size: ',training_data.data.size())\n",
    "print('Training data targets size: ' , training_data.targets.size())\n",
    "plt.imshow(image.reshape((28,28)), cmap=\"gray\")\n",
    "print('Shape: ', image.shape)\n",
    "print('Label: ', label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is starting\n",
      "Batch 100/12000: train_loss = 2.27473\n",
      "Batch 200/12000: train_loss = 2.25125\n",
      "Batch 300/12000: train_loss = 2.21525\n",
      "Batch 400/12000: train_loss = 2.15370\n",
      "Batch 500/12000: train_loss = 2.05963\n",
      "Batch 600/12000: train_loss = 1.93583\n",
      "Batch 700/12000: train_loss = 1.81384\n",
      "Batch 800/12000: train_loss = 1.69458\n",
      "Batch 900/12000: train_loss = 1.58692\n",
      "Batch 1000/12000: train_loss = 1.49010\n",
      "Batch 1100/12000: train_loss = 1.40973\n",
      "Batch 1200/12000: train_loss = 1.34043\n",
      "Batch 1300/12000: train_loss = 1.27093\n",
      "Batch 1400/12000: train_loss = 1.21515\n",
      "Batch 1500/12000: train_loss = 1.17130\n",
      "Batch 1600/12000: train_loss = 1.13060\n",
      "Batch 1700/12000: train_loss = 1.09342\n",
      "Batch 1800/12000: train_loss = 1.06318\n",
      "Batch 1900/12000: train_loss = 1.03111\n",
      "Batch 2000/12000: train_loss = 0.99754\n",
      "Batch 2100/12000: train_loss = 0.96763\n",
      "Batch 2200/12000: train_loss = 0.93845\n",
      "Batch 2300/12000: train_loss = 0.91211\n",
      "Batch 2400/12000: train_loss = 0.89411\n",
      "Batch 2500/12000: train_loss = 0.87288\n",
      "Batch 2600/12000: train_loss = 0.85778\n",
      "Batch 2700/12000: train_loss = 0.84263\n",
      "Batch 2800/12000: train_loss = 0.82815\n",
      "Batch 2900/12000: train_loss = 0.81485\n",
      "Batch 3000/12000: train_loss = 0.80324\n",
      "Batch 3100/12000: train_loss = 0.78803\n",
      "Batch 3200/12000: train_loss = 0.77566\n",
      "Batch 3300/12000: train_loss = 0.76226\n",
      "Batch 3400/12000: train_loss = 0.75169\n",
      "Batch 3500/12000: train_loss = 0.74062\n",
      "Batch 3600/12000: train_loss = 0.73122\n",
      "Batch 3700/12000: train_loss = 0.71880\n",
      "Batch 3800/12000: train_loss = 0.70783\n",
      "Batch 3900/12000: train_loss = 0.69745\n",
      "Batch 4000/12000: train_loss = 0.68648\n",
      "Batch 4100/12000: train_loss = 0.67758\n",
      "Batch 4200/12000: train_loss = 0.67162\n",
      "Batch 4300/12000: train_loss = 0.66280\n",
      "Batch 4400/12000: train_loss = 0.65338\n",
      "Batch 4500/12000: train_loss = 0.64547\n",
      "Batch 4600/12000: train_loss = 0.63825\n",
      "Batch 4700/12000: train_loss = 0.63050\n",
      "Batch 4800/12000: train_loss = 0.62439\n",
      "Batch 4900/12000: train_loss = 0.61763\n",
      "Batch 5000/12000: train_loss = 0.61268\n",
      "Batch 5100/12000: train_loss = 0.60542\n",
      "Batch 5200/12000: train_loss = 0.59874\n",
      "Batch 5300/12000: train_loss = 0.59302\n",
      "Batch 5400/12000: train_loss = 0.58896\n",
      "Batch 5500/12000: train_loss = 0.58354\n",
      "Batch 5600/12000: train_loss = 0.57763\n",
      "Batch 5700/12000: train_loss = 0.57217\n",
      "Batch 5800/12000: train_loss = 0.56741\n",
      "Batch 5900/12000: train_loss = 0.56309\n",
      "Batch 6000/12000: train_loss = 0.55911\n",
      "Batch 6100/12000: train_loss = 0.55462\n",
      "Batch 6200/12000: train_loss = 0.55062\n",
      "Batch 6300/12000: train_loss = 0.54807\n",
      "Batch 6400/12000: train_loss = 0.54440\n",
      "Batch 6500/12000: train_loss = 0.54129\n",
      "Batch 6600/12000: train_loss = 0.53688\n",
      "Batch 6700/12000: train_loss = 0.53254\n",
      "Batch 6800/12000: train_loss = 0.52725\n",
      "Batch 6900/12000: train_loss = 0.52274\n",
      "Batch 7000/12000: train_loss = 0.51975\n",
      "Batch 7100/12000: train_loss = 0.51580\n",
      "Batch 7200/12000: train_loss = 0.51129\n",
      "Batch 7300/12000: train_loss = 0.50741\n",
      "Batch 7400/12000: train_loss = 0.50297\n",
      "Batch 7500/12000: train_loss = 0.50133\n",
      "Batch 7600/12000: train_loss = 0.49846\n",
      "Batch 7700/12000: train_loss = 0.49462\n",
      "Batch 7800/12000: train_loss = 0.49144\n",
      "Batch 7900/12000: train_loss = 0.48897\n",
      "Batch 8000/12000: train_loss = 0.48579\n",
      "Batch 8100/12000: train_loss = 0.48258\n",
      "Batch 8200/12000: train_loss = 0.47920\n",
      "Batch 8300/12000: train_loss = 0.47700\n",
      "Batch 8400/12000: train_loss = 0.47421\n",
      "Batch 8500/12000: train_loss = 0.47205\n",
      "Batch 8600/12000: train_loss = 0.46985\n",
      "Batch 8700/12000: train_loss = 0.46651\n",
      "Batch 8800/12000: train_loss = 0.46377\n",
      "Batch 8900/12000: train_loss = 0.46129\n",
      "Batch 9000/12000: train_loss = 0.45841\n",
      "Batch 9100/12000: train_loss = 0.45604\n",
      "Batch 9200/12000: train_loss = 0.45421\n",
      "Batch 9300/12000: train_loss = 0.45267\n",
      "Batch 9400/12000: train_loss = 0.44962\n",
      "Batch 9500/12000: train_loss = 0.44742\n",
      "Batch 9600/12000: train_loss = 0.44530\n",
      "Batch 9700/12000: train_loss = 0.44245\n",
      "Batch 9800/12000: train_loss = 0.44005\n",
      "Batch 9900/12000: train_loss = 0.43821\n",
      "Batch 10000/12000: train_loss = 0.43653\n",
      "Batch 10100/12000: train_loss = 0.43525\n",
      "Batch 10200/12000: train_loss = 0.43299\n",
      "Batch 10300/12000: train_loss = 0.43037\n",
      "Batch 10400/12000: train_loss = 0.42784\n",
      "Batch 10500/12000: train_loss = 0.42604\n",
      "Batch 10600/12000: train_loss = 0.42490\n",
      "Batch 10700/12000: train_loss = 0.42233\n",
      "Batch 10800/12000: train_loss = 0.42060\n",
      "Batch 10900/12000: train_loss = 0.41824\n",
      "Batch 11000/12000: train_loss = 0.41638\n",
      "Batch 11100/12000: train_loss = 0.41422\n",
      "Batch 11200/12000: train_loss = 0.41201\n",
      "Batch 11300/12000: train_loss = 0.41017\n",
      "Batch 11400/12000: train_loss = 0.40769\n",
      "Batch 11500/12000: train_loss = 0.40523\n",
      "Batch 11600/12000: train_loss = 0.40311\n",
      "Batch 11700/12000: train_loss = 0.40056\n",
      "Batch 11800/12000: train_loss = 0.39804\n",
      "Batch 11900/12000: train_loss = 0.39554\n",
      "Batch 12000/12000: train_loss = 0.39382\n",
      "Epoch 1 completed. Average training loss: 0.39382\n",
      "Epoch 2 is starting\n",
      "Batch 100/12000: train_loss = 0.17439\n",
      "Batch 200/12000: train_loss = 0.20056\n",
      "Batch 300/12000: train_loss = 0.22076\n",
      "Batch 400/12000: train_loss = 0.20422\n",
      "Batch 500/12000: train_loss = 0.18921\n",
      "Batch 600/12000: train_loss = 0.18129\n",
      "Batch 700/12000: train_loss = 0.17404\n",
      "Batch 800/12000: train_loss = 0.17158\n",
      "Batch 900/12000: train_loss = 0.17039\n",
      "Batch 1000/12000: train_loss = 0.16968\n",
      "Batch 1100/12000: train_loss = 0.16941\n",
      "Batch 1200/12000: train_loss = 0.16933\n",
      "Batch 1300/12000: train_loss = 0.16671\n",
      "Batch 1400/12000: train_loss = 0.16734\n",
      "Batch 1500/12000: train_loss = 0.17026\n",
      "Batch 1600/12000: train_loss = 0.17069\n",
      "Batch 1700/12000: train_loss = 0.17261\n",
      "Batch 1800/12000: train_loss = 0.17600\n",
      "Batch 1900/12000: train_loss = 0.17674\n",
      "Batch 2000/12000: train_loss = 0.17354\n",
      "Batch 2100/12000: train_loss = 0.17249\n",
      "Batch 2200/12000: train_loss = 0.17091\n",
      "Batch 2300/12000: train_loss = 0.16845\n",
      "Batch 2400/12000: train_loss = 0.16957\n",
      "Batch 2500/12000: train_loss = 0.16874\n",
      "Batch 2600/12000: train_loss = 0.17072\n",
      "Batch 2700/12000: train_loss = 0.17109\n",
      "Batch 2800/12000: train_loss = 0.17135\n",
      "Batch 2900/12000: train_loss = 0.17196\n",
      "Batch 3000/12000: train_loss = 0.17263\n",
      "Batch 3100/12000: train_loss = 0.17234\n",
      "Batch 3200/12000: train_loss = 0.17216\n",
      "Batch 3300/12000: train_loss = 0.17125\n",
      "Batch 3400/12000: train_loss = 0.17096\n",
      "Batch 3500/12000: train_loss = 0.17037\n",
      "Batch 3600/12000: train_loss = 0.17038\n",
      "Batch 3700/12000: train_loss = 0.16912\n",
      "Batch 3800/12000: train_loss = 0.16806\n",
      "Batch 3900/12000: train_loss = 0.16739\n",
      "Batch 4000/12000: train_loss = 0.16664\n",
      "Batch 4100/12000: train_loss = 0.16633\n",
      "Batch 4200/12000: train_loss = 0.16769\n",
      "Batch 4300/12000: train_loss = 0.16708\n",
      "Batch 4400/12000: train_loss = 0.16612\n",
      "Batch 4500/12000: train_loss = 0.16593\n",
      "Batch 4600/12000: train_loss = 0.16556\n",
      "Batch 4700/12000: train_loss = 0.16465\n",
      "Batch 4800/12000: train_loss = 0.16481\n",
      "Batch 4900/12000: train_loss = 0.16391\n",
      "Batch 5000/12000: train_loss = 0.16423\n",
      "Batch 5100/12000: train_loss = 0.16310\n",
      "Batch 5200/12000: train_loss = 0.16294\n",
      "Batch 5300/12000: train_loss = 0.16263\n",
      "Batch 5400/12000: train_loss = 0.16368\n",
      "Batch 5500/12000: train_loss = 0.16338\n",
      "Batch 5600/12000: train_loss = 0.16281\n",
      "Batch 5700/12000: train_loss = 0.16249\n",
      "Batch 5800/12000: train_loss = 0.16222\n",
      "Batch 5900/12000: train_loss = 0.16190\n",
      "Batch 6000/12000: train_loss = 0.16159\n",
      "Batch 6100/12000: train_loss = 0.16113\n",
      "Batch 6200/12000: train_loss = 0.16078\n",
      "Batch 6300/12000: train_loss = 0.16138\n",
      "Batch 6400/12000: train_loss = 0.16153\n",
      "Batch 6500/12000: train_loss = 0.16185\n",
      "Batch 6600/12000: train_loss = 0.16144\n",
      "Batch 6700/12000: train_loss = 0.16089\n",
      "Batch 6800/12000: train_loss = 0.15985\n",
      "Batch 6900/12000: train_loss = 0.15926\n",
      "Batch 7000/12000: train_loss = 0.15958\n",
      "Batch 7100/12000: train_loss = 0.15940\n",
      "Batch 7200/12000: train_loss = 0.15845\n",
      "Batch 7300/12000: train_loss = 0.15819\n",
      "Batch 7400/12000: train_loss = 0.15745\n",
      "Batch 7500/12000: train_loss = 0.15814\n",
      "Batch 7600/12000: train_loss = 0.15809\n",
      "Batch 7700/12000: train_loss = 0.15721\n",
      "Batch 7800/12000: train_loss = 0.15680\n",
      "Batch 7900/12000: train_loss = 0.15681\n",
      "Batch 8000/12000: train_loss = 0.15651\n",
      "Batch 8100/12000: train_loss = 0.15598\n",
      "Batch 8200/12000: train_loss = 0.15560\n",
      "Batch 8300/12000: train_loss = 0.15609\n",
      "Batch 8400/12000: train_loss = 0.15579\n",
      "Batch 8500/12000: train_loss = 0.15592\n",
      "Batch 8600/12000: train_loss = 0.15617\n",
      "Batch 8700/12000: train_loss = 0.15537\n",
      "Batch 8800/12000: train_loss = 0.15504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8900/12000: train_loss = 0.15474\n",
      "Batch 9000/12000: train_loss = 0.15416\n",
      "Batch 9100/12000: train_loss = 0.15382\n",
      "Batch 9200/12000: train_loss = 0.15410\n",
      "Batch 9300/12000: train_loss = 0.15455\n",
      "Batch 9400/12000: train_loss = 0.15397\n",
      "Batch 9500/12000: train_loss = 0.15394\n",
      "Batch 9600/12000: train_loss = 0.15394\n",
      "Batch 9700/12000: train_loss = 0.15335\n",
      "Batch 9800/12000: train_loss = 0.15302\n",
      "Batch 9900/12000: train_loss = 0.15303\n",
      "Batch 10000/12000: train_loss = 0.15321\n",
      "Batch 10100/12000: train_loss = 0.15334\n",
      "Batch 10200/12000: train_loss = 0.15307\n",
      "Batch 10300/12000: train_loss = 0.15256\n",
      "Batch 10400/12000: train_loss = 0.15215\n",
      "Batch 10500/12000: train_loss = 0.15194\n",
      "Batch 10600/12000: train_loss = 0.15218\n",
      "Batch 10700/12000: train_loss = 0.15154\n",
      "Batch 10800/12000: train_loss = 0.15150\n",
      "Batch 10900/12000: train_loss = 0.15083\n",
      "Batch 11000/12000: train_loss = 0.15054\n",
      "Batch 11100/12000: train_loss = 0.15023\n",
      "Batch 11200/12000: train_loss = 0.14988\n",
      "Batch 11300/12000: train_loss = 0.14964\n",
      "Batch 11400/12000: train_loss = 0.14891\n",
      "Batch 11500/12000: train_loss = 0.14830\n",
      "Batch 11600/12000: train_loss = 0.14778\n",
      "Batch 11700/12000: train_loss = 0.14698\n",
      "Batch 11800/12000: train_loss = 0.14618\n",
      "Batch 11900/12000: train_loss = 0.14527\n",
      "Batch 12000/12000: train_loss = 0.14531\n",
      "Epoch 2 completed. Average training loss: 0.14531\n",
      "Epoch 3 is starting\n",
      "Batch 100/12000: train_loss = 0.10824\n",
      "Batch 200/12000: train_loss = 0.12580\n",
      "Batch 300/12000: train_loss = 0.14107\n",
      "Batch 400/12000: train_loss = 0.12950\n",
      "Batch 500/12000: train_loss = 0.11725\n",
      "Batch 600/12000: train_loss = 0.11243\n",
      "Batch 700/12000: train_loss = 0.10613\n",
      "Batch 800/12000: train_loss = 0.10384\n",
      "Batch 900/12000: train_loss = 0.10242\n",
      "Batch 1000/12000: train_loss = 0.10169\n",
      "Batch 1100/12000: train_loss = 0.10114\n",
      "Batch 1200/12000: train_loss = 0.10164\n",
      "Batch 1300/12000: train_loss = 0.10105\n",
      "Batch 1400/12000: train_loss = 0.10186\n",
      "Batch 1500/12000: train_loss = 0.10412\n",
      "Batch 1600/12000: train_loss = 0.10450\n",
      "Batch 1700/12000: train_loss = 0.10635\n",
      "Batch 1800/12000: train_loss = 0.10783\n",
      "Batch 1900/12000: train_loss = 0.10818\n",
      "Batch 2000/12000: train_loss = 0.10583\n",
      "Batch 2100/12000: train_loss = 0.10574\n",
      "Batch 2200/12000: train_loss = 0.10477\n",
      "Batch 2300/12000: train_loss = 0.10322\n",
      "Batch 2400/12000: train_loss = 0.10392\n",
      "Batch 2500/12000: train_loss = 0.10310\n",
      "Batch 2600/12000: train_loss = 0.10368\n",
      "Batch 2700/12000: train_loss = 0.10344\n",
      "Batch 2800/12000: train_loss = 0.10360\n",
      "Batch 2900/12000: train_loss = 0.10353\n",
      "Batch 3000/12000: train_loss = 0.10350\n",
      "Batch 3100/12000: train_loss = 0.10374\n",
      "Batch 3200/12000: train_loss = 0.10383\n",
      "Batch 3300/12000: train_loss = 0.10338\n",
      "Batch 3400/12000: train_loss = 0.10342\n",
      "Batch 3500/12000: train_loss = 0.10320\n",
      "Batch 3600/12000: train_loss = 0.10340\n",
      "Batch 3700/12000: train_loss = 0.10290\n",
      "Batch 3800/12000: train_loss = 0.10199\n",
      "Batch 3900/12000: train_loss = 0.10202\n",
      "Batch 4000/12000: train_loss = 0.10151\n",
      "Batch 4100/12000: train_loss = 0.10154\n",
      "Batch 4200/12000: train_loss = 0.10252\n",
      "Batch 4300/12000: train_loss = 0.10193\n",
      "Batch 4400/12000: train_loss = 0.10154\n",
      "Batch 4500/12000: train_loss = 0.10161\n",
      "Batch 4600/12000: train_loss = 0.10128\n",
      "Batch 4700/12000: train_loss = 0.10076\n",
      "Batch 4800/12000: train_loss = 0.10102\n",
      "Batch 4900/12000: train_loss = 0.10032\n",
      "Batch 5000/12000: train_loss = 0.10047\n",
      "Batch 5100/12000: train_loss = 0.09986\n",
      "Batch 5200/12000: train_loss = 0.10008\n",
      "Batch 5300/12000: train_loss = 0.10011\n",
      "Batch 5400/12000: train_loss = 0.10136\n",
      "Batch 5500/12000: train_loss = 0.10132\n",
      "Batch 5600/12000: train_loss = 0.10121\n",
      "Batch 5700/12000: train_loss = 0.10104\n",
      "Batch 5800/12000: train_loss = 0.10101\n",
      "Batch 5900/12000: train_loss = 0.10091\n",
      "Batch 6000/12000: train_loss = 0.10046\n",
      "Batch 6100/12000: train_loss = 0.10012\n",
      "Batch 6200/12000: train_loss = 0.09988\n",
      "Batch 6300/12000: train_loss = 0.10026\n",
      "Batch 6400/12000: train_loss = 0.10057\n",
      "Batch 6500/12000: train_loss = 0.10092\n",
      "Batch 6600/12000: train_loss = 0.10076\n",
      "Batch 6700/12000: train_loss = 0.10052\n",
      "Batch 6800/12000: train_loss = 0.09998\n",
      "Batch 6900/12000: train_loss = 0.09979\n",
      "Batch 7000/12000: train_loss = 0.10012\n",
      "Batch 7100/12000: train_loss = 0.10021\n",
      "Batch 7200/12000: train_loss = 0.09970\n",
      "Batch 7300/12000: train_loss = 0.09968\n",
      "Batch 7400/12000: train_loss = 0.09919\n",
      "Batch 7500/12000: train_loss = 0.09971\n",
      "Batch 7600/12000: train_loss = 0.09976\n",
      "Batch 7700/12000: train_loss = 0.09920\n",
      "Batch 7800/12000: train_loss = 0.09898\n",
      "Batch 7900/12000: train_loss = 0.09922\n",
      "Batch 8000/12000: train_loss = 0.09910\n",
      "Batch 8100/12000: train_loss = 0.09882\n",
      "Batch 8200/12000: train_loss = 0.09870\n",
      "Batch 8300/12000: train_loss = 0.09914\n",
      "Batch 8400/12000: train_loss = 0.09914\n",
      "Batch 8500/12000: train_loss = 0.09920\n",
      "Batch 8600/12000: train_loss = 0.09951\n",
      "Batch 8700/12000: train_loss = 0.09896\n",
      "Batch 8800/12000: train_loss = 0.09888\n",
      "Batch 8900/12000: train_loss = 0.09883\n",
      "Batch 9000/12000: train_loss = 0.09842\n",
      "Batch 9100/12000: train_loss = 0.09821\n",
      "Batch 9200/12000: train_loss = 0.09850\n",
      "Batch 9300/12000: train_loss = 0.09892\n",
      "Batch 9400/12000: train_loss = 0.09862\n",
      "Batch 9500/12000: train_loss = 0.09874\n",
      "Batch 9600/12000: train_loss = 0.09893\n",
      "Batch 9700/12000: train_loss = 0.09854\n",
      "Batch 9800/12000: train_loss = 0.09840\n",
      "Batch 9900/12000: train_loss = 0.09858\n",
      "Batch 10000/12000: train_loss = 0.09884\n",
      "Batch 10100/12000: train_loss = 0.09894\n",
      "Batch 10200/12000: train_loss = 0.09879\n",
      "Batch 10300/12000: train_loss = 0.09855\n",
      "Batch 10400/12000: train_loss = 0.09835\n",
      "Batch 10500/12000: train_loss = 0.09826\n",
      "Batch 10600/12000: train_loss = 0.09848\n",
      "Batch 10700/12000: train_loss = 0.09810\n",
      "Batch 10800/12000: train_loss = 0.09812\n",
      "Batch 10900/12000: train_loss = 0.09772\n",
      "Batch 11000/12000: train_loss = 0.09758\n",
      "Batch 11100/12000: train_loss = 0.09746\n",
      "Batch 11200/12000: train_loss = 0.09729\n",
      "Batch 11300/12000: train_loss = 0.09719\n",
      "Batch 11400/12000: train_loss = 0.09672\n",
      "Batch 11500/12000: train_loss = 0.09640\n",
      "Batch 11600/12000: train_loss = 0.09609\n",
      "Batch 11700/12000: train_loss = 0.09556\n",
      "Batch 11800/12000: train_loss = 0.09505\n",
      "Batch 11900/12000: train_loss = 0.09443\n",
      "Batch 12000/12000: train_loss = 0.09468\n",
      "Epoch 3 completed. Average training loss: 0.09468\n",
      "Epoch 4 is starting\n",
      "Batch 100/12000: train_loss = 0.07753\n",
      "Batch 200/12000: train_loss = 0.09184\n",
      "Batch 300/12000: train_loss = 0.10166\n",
      "Batch 400/12000: train_loss = 0.09389\n",
      "Batch 500/12000: train_loss = 0.08376\n",
      "Batch 600/12000: train_loss = 0.08045\n",
      "Batch 700/12000: train_loss = 0.07545\n",
      "Batch 800/12000: train_loss = 0.07352\n",
      "Batch 900/12000: train_loss = 0.07229\n",
      "Batch 1000/12000: train_loss = 0.07150\n",
      "Batch 1100/12000: train_loss = 0.07139\n",
      "Batch 1200/12000: train_loss = 0.07172\n",
      "Batch 1300/12000: train_loss = 0.07097\n",
      "Batch 1400/12000: train_loss = 0.07169\n",
      "Batch 1500/12000: train_loss = 0.07318\n",
      "Batch 1600/12000: train_loss = 0.07392\n",
      "Batch 1700/12000: train_loss = 0.07539\n",
      "Batch 1800/12000: train_loss = 0.07637\n",
      "Batch 1900/12000: train_loss = 0.07645\n",
      "Batch 2000/12000: train_loss = 0.07462\n",
      "Batch 2100/12000: train_loss = 0.07468\n",
      "Batch 2200/12000: train_loss = 0.07380\n",
      "Batch 2300/12000: train_loss = 0.07271\n",
      "Batch 2400/12000: train_loss = 0.07323\n",
      "Batch 2500/12000: train_loss = 0.07228\n",
      "Batch 2600/12000: train_loss = 0.07241\n",
      "Batch 2700/12000: train_loss = 0.07214\n",
      "Batch 2800/12000: train_loss = 0.07245\n",
      "Batch 2900/12000: train_loss = 0.07205\n",
      "Batch 3000/12000: train_loss = 0.07193\n",
      "Batch 3100/12000: train_loss = 0.07208\n",
      "Batch 3200/12000: train_loss = 0.07228\n",
      "Batch 3300/12000: train_loss = 0.07192\n",
      "Batch 3400/12000: train_loss = 0.07202\n",
      "Batch 3500/12000: train_loss = 0.07189\n",
      "Batch 3600/12000: train_loss = 0.07222\n",
      "Batch 3700/12000: train_loss = 0.07203\n",
      "Batch 3800/12000: train_loss = 0.07123\n",
      "Batch 3900/12000: train_loss = 0.07154\n",
      "Batch 4000/12000: train_loss = 0.07105\n",
      "Batch 4100/12000: train_loss = 0.07102\n",
      "Batch 4200/12000: train_loss = 0.07176\n",
      "Batch 4300/12000: train_loss = 0.07137\n",
      "Batch 4400/12000: train_loss = 0.07120\n",
      "Batch 4500/12000: train_loss = 0.07124\n",
      "Batch 4600/12000: train_loss = 0.07104\n",
      "Batch 4700/12000: train_loss = 0.07065\n",
      "Batch 4800/12000: train_loss = 0.07088\n",
      "Batch 4900/12000: train_loss = 0.07030\n",
      "Batch 5000/12000: train_loss = 0.07034\n",
      "Batch 5100/12000: train_loss = 0.06995\n",
      "Batch 5200/12000: train_loss = 0.07026\n",
      "Batch 5300/12000: train_loss = 0.07029\n",
      "Batch 5400/12000: train_loss = 0.07149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/12000: train_loss = 0.07153\n",
      "Batch 5600/12000: train_loss = 0.07155\n",
      "Batch 5700/12000: train_loss = 0.07146\n",
      "Batch 5800/12000: train_loss = 0.07147\n",
      "Batch 5900/12000: train_loss = 0.07138\n",
      "Batch 6000/12000: train_loss = 0.07096\n",
      "Batch 6100/12000: train_loss = 0.07064\n",
      "Batch 6200/12000: train_loss = 0.07044\n",
      "Batch 6300/12000: train_loss = 0.07073\n",
      "Batch 6400/12000: train_loss = 0.07110\n",
      "Batch 6500/12000: train_loss = 0.07138\n",
      "Batch 6600/12000: train_loss = 0.07131\n",
      "Batch 6700/12000: train_loss = 0.07113\n",
      "Batch 6800/12000: train_loss = 0.07082\n",
      "Batch 6900/12000: train_loss = 0.07073\n",
      "Batch 7000/12000: train_loss = 0.07098\n",
      "Batch 7100/12000: train_loss = 0.07123\n",
      "Batch 7200/12000: train_loss = 0.07094\n",
      "Batch 7300/12000: train_loss = 0.07099\n",
      "Batch 7400/12000: train_loss = 0.07064\n",
      "Batch 7500/12000: train_loss = 0.07103\n",
      "Batch 7600/12000: train_loss = 0.07108\n",
      "Batch 7700/12000: train_loss = 0.07071\n",
      "Batch 7800/12000: train_loss = 0.07056\n",
      "Batch 7900/12000: train_loss = 0.07087\n",
      "Batch 8000/12000: train_loss = 0.07079\n",
      "Batch 8100/12000: train_loss = 0.07061\n",
      "Batch 8200/12000: train_loss = 0.07058\n",
      "Batch 8300/12000: train_loss = 0.07089\n",
      "Batch 8400/12000: train_loss = 0.07093\n",
      "Batch 8500/12000: train_loss = 0.07097\n",
      "Batch 8600/12000: train_loss = 0.07131\n",
      "Batch 8700/12000: train_loss = 0.07093\n",
      "Batch 8800/12000: train_loss = 0.07097\n",
      "Batch 8900/12000: train_loss = 0.07102\n",
      "Batch 9000/12000: train_loss = 0.07068\n",
      "Batch 9100/12000: train_loss = 0.07051\n",
      "Batch 9200/12000: train_loss = 0.07075\n",
      "Batch 9300/12000: train_loss = 0.07109\n",
      "Batch 9400/12000: train_loss = 0.07084\n",
      "Batch 9500/12000: train_loss = 0.07095\n",
      "Batch 9600/12000: train_loss = 0.07117\n",
      "Batch 9700/12000: train_loss = 0.07086\n",
      "Batch 9800/12000: train_loss = 0.07077\n",
      "Batch 9900/12000: train_loss = 0.07099\n",
      "Batch 10000/12000: train_loss = 0.07123\n",
      "Batch 10100/12000: train_loss = 0.07131\n",
      "Batch 10200/12000: train_loss = 0.07117\n",
      "Batch 10300/12000: train_loss = 0.07105\n",
      "Batch 10400/12000: train_loss = 0.07094\n",
      "Batch 10500/12000: train_loss = 0.07087\n",
      "Batch 10600/12000: train_loss = 0.07107\n",
      "Batch 10700/12000: train_loss = 0.07083\n",
      "Batch 10800/12000: train_loss = 0.07083\n",
      "Batch 10900/12000: train_loss = 0.07057\n",
      "Batch 11000/12000: train_loss = 0.07049\n",
      "Batch 11100/12000: train_loss = 0.07041\n",
      "Batch 11200/12000: train_loss = 0.07027\n",
      "Batch 11300/12000: train_loss = 0.07024\n",
      "Batch 11400/12000: train_loss = 0.06989\n",
      "Batch 11500/12000: train_loss = 0.06973\n",
      "Batch 11600/12000: train_loss = 0.06949\n",
      "Batch 11700/12000: train_loss = 0.06911\n",
      "Batch 11800/12000: train_loss = 0.06874\n",
      "Batch 11900/12000: train_loss = 0.06828\n",
      "Batch 12000/12000: train_loss = 0.06861\n",
      "Epoch 4 completed. Average training loss: 0.06861\n",
      "Epoch 5 is starting\n",
      "Batch 100/12000: train_loss = 0.06134\n",
      "Batch 200/12000: train_loss = 0.07147\n",
      "Batch 300/12000: train_loss = 0.07766\n",
      "Batch 400/12000: train_loss = 0.07263\n",
      "Batch 500/12000: train_loss = 0.06391\n",
      "Batch 600/12000: train_loss = 0.06131\n",
      "Batch 700/12000: train_loss = 0.05749\n",
      "Batch 800/12000: train_loss = 0.05610\n",
      "Batch 900/12000: train_loss = 0.05489\n",
      "Batch 1000/12000: train_loss = 0.05394\n",
      "Batch 1100/12000: train_loss = 0.05446\n",
      "Batch 1200/12000: train_loss = 0.05485\n",
      "Batch 1300/12000: train_loss = 0.05379\n",
      "Batch 1400/12000: train_loss = 0.05439\n",
      "Batch 1500/12000: train_loss = 0.05520\n",
      "Batch 1600/12000: train_loss = 0.05620\n",
      "Batch 1700/12000: train_loss = 0.05735\n",
      "Batch 1800/12000: train_loss = 0.05802\n",
      "Batch 1900/12000: train_loss = 0.05831\n",
      "Batch 2000/12000: train_loss = 0.05687\n",
      "Batch 2100/12000: train_loss = 0.05671\n",
      "Batch 2200/12000: train_loss = 0.05603\n",
      "Batch 2300/12000: train_loss = 0.05512\n",
      "Batch 2400/12000: train_loss = 0.05545\n",
      "Batch 2500/12000: train_loss = 0.05457\n",
      "Batch 2600/12000: train_loss = 0.05454\n",
      "Batch 2700/12000: train_loss = 0.05412\n",
      "Batch 2800/12000: train_loss = 0.05440\n",
      "Batch 2900/12000: train_loss = 0.05390\n",
      "Batch 3000/12000: train_loss = 0.05372\n",
      "Batch 3100/12000: train_loss = 0.05377\n",
      "Batch 3200/12000: train_loss = 0.05398\n",
      "Batch 3300/12000: train_loss = 0.05357\n",
      "Batch 3400/12000: train_loss = 0.05360\n",
      "Batch 3500/12000: train_loss = 0.05343\n",
      "Batch 3600/12000: train_loss = 0.05378\n",
      "Batch 3700/12000: train_loss = 0.05372\n",
      "Batch 3800/12000: train_loss = 0.05308\n",
      "Batch 3900/12000: train_loss = 0.05339\n",
      "Batch 4000/12000: train_loss = 0.05296\n",
      "Batch 4100/12000: train_loss = 0.05290\n",
      "Batch 4200/12000: train_loss = 0.05343\n",
      "Batch 4300/12000: train_loss = 0.05319\n",
      "Batch 4400/12000: train_loss = 0.05306\n",
      "Batch 4500/12000: train_loss = 0.05311\n",
      "Batch 4600/12000: train_loss = 0.05293\n",
      "Batch 4700/12000: train_loss = 0.05261\n",
      "Batch 4800/12000: train_loss = 0.05285\n",
      "Batch 4900/12000: train_loss = 0.05237\n",
      "Batch 5000/12000: train_loss = 0.05231\n",
      "Batch 5100/12000: train_loss = 0.05202\n",
      "Batch 5200/12000: train_loss = 0.05236\n",
      "Batch 5300/12000: train_loss = 0.05239\n",
      "Batch 5400/12000: train_loss = 0.05348\n",
      "Batch 5500/12000: train_loss = 0.05360\n",
      "Batch 5600/12000: train_loss = 0.05365\n",
      "Batch 5700/12000: train_loss = 0.05357\n",
      "Batch 5800/12000: train_loss = 0.05353\n",
      "Batch 5900/12000: train_loss = 0.05349\n",
      "Batch 6000/12000: train_loss = 0.05314\n",
      "Batch 6100/12000: train_loss = 0.05281\n",
      "Batch 6200/12000: train_loss = 0.05260\n",
      "Batch 6300/12000: train_loss = 0.05287\n",
      "Batch 6400/12000: train_loss = 0.05318\n",
      "Batch 6500/12000: train_loss = 0.05337\n",
      "Batch 6600/12000: train_loss = 0.05327\n",
      "Batch 6700/12000: train_loss = 0.05310\n",
      "Batch 6800/12000: train_loss = 0.05287\n",
      "Batch 6900/12000: train_loss = 0.05284\n",
      "Batch 7000/12000: train_loss = 0.05303\n",
      "Batch 7100/12000: train_loss = 0.05336\n",
      "Batch 7200/12000: train_loss = 0.05319\n",
      "Batch 7300/12000: train_loss = 0.05331\n",
      "Batch 7400/12000: train_loss = 0.05303\n",
      "Batch 7500/12000: train_loss = 0.05332\n",
      "Batch 7600/12000: train_loss = 0.05338\n",
      "Batch 7700/12000: train_loss = 0.05316\n",
      "Batch 7800/12000: train_loss = 0.05305\n",
      "Batch 7900/12000: train_loss = 0.05334\n",
      "Batch 8000/12000: train_loss = 0.05326\n",
      "Batch 8100/12000: train_loss = 0.05307\n",
      "Batch 8200/12000: train_loss = 0.05310\n",
      "Batch 8300/12000: train_loss = 0.05333\n",
      "Batch 8400/12000: train_loss = 0.05340\n",
      "Batch 8500/12000: train_loss = 0.05342\n",
      "Batch 8600/12000: train_loss = 0.05377\n",
      "Batch 8700/12000: train_loss = 0.05348\n",
      "Batch 8800/12000: train_loss = 0.05355\n",
      "Batch 8900/12000: train_loss = 0.05360\n",
      "Batch 9000/12000: train_loss = 0.05331\n",
      "Batch 9100/12000: train_loss = 0.05318\n",
      "Batch 9200/12000: train_loss = 0.05334\n",
      "Batch 9300/12000: train_loss = 0.05358\n",
      "Batch 9400/12000: train_loss = 0.05338\n",
      "Batch 9500/12000: train_loss = 0.05346\n",
      "Batch 9600/12000: train_loss = 0.05368\n",
      "Batch 9700/12000: train_loss = 0.05345\n",
      "Batch 9800/12000: train_loss = 0.05338\n",
      "Batch 9900/12000: train_loss = 0.05358\n",
      "Batch 10000/12000: train_loss = 0.05375\n",
      "Batch 10100/12000: train_loss = 0.05381\n",
      "Batch 10200/12000: train_loss = 0.05370\n",
      "Batch 10300/12000: train_loss = 0.05365\n",
      "Batch 10400/12000: train_loss = 0.05358\n",
      "Batch 10500/12000: train_loss = 0.05354\n",
      "Batch 10600/12000: train_loss = 0.05369\n",
      "Batch 10700/12000: train_loss = 0.05353\n",
      "Batch 10800/12000: train_loss = 0.05351\n",
      "Batch 10900/12000: train_loss = 0.05335\n",
      "Batch 11000/12000: train_loss = 0.05330\n",
      "Batch 11100/12000: train_loss = 0.05323\n",
      "Batch 11200/12000: train_loss = 0.05312\n",
      "Batch 11300/12000: train_loss = 0.05309\n",
      "Batch 11400/12000: train_loss = 0.05282\n",
      "Batch 11500/12000: train_loss = 0.05272\n",
      "Batch 11600/12000: train_loss = 0.05252\n",
      "Batch 11700/12000: train_loss = 0.05224\n",
      "Batch 11800/12000: train_loss = 0.05197\n",
      "Batch 11900/12000: train_loss = 0.05162\n",
      "Batch 12000/12000: train_loss = 0.05199\n",
      "Epoch 5 completed. Average training loss: 0.05199\n",
      "Epoch 6 is starting\n",
      "Batch 100/12000: train_loss = 0.04998\n",
      "Batch 200/12000: train_loss = 0.05710\n",
      "Batch 300/12000: train_loss = 0.06134\n",
      "Batch 400/12000: train_loss = 0.05798\n",
      "Batch 500/12000: train_loss = 0.05064\n",
      "Batch 600/12000: train_loss = 0.04867\n",
      "Batch 700/12000: train_loss = 0.04519\n",
      "Batch 800/12000: train_loss = 0.04403\n",
      "Batch 900/12000: train_loss = 0.04303\n",
      "Batch 1000/12000: train_loss = 0.04229\n",
      "Batch 1100/12000: train_loss = 0.04319\n",
      "Batch 1200/12000: train_loss = 0.04366\n",
      "Batch 1300/12000: train_loss = 0.04235\n",
      "Batch 1400/12000: train_loss = 0.04285\n",
      "Batch 1500/12000: train_loss = 0.04324\n",
      "Batch 1600/12000: train_loss = 0.04413\n",
      "Batch 1700/12000: train_loss = 0.04512\n",
      "Batch 1800/12000: train_loss = 0.04545\n",
      "Batch 1900/12000: train_loss = 0.04563\n",
      "Batch 2000/12000: train_loss = 0.04446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2100/12000: train_loss = 0.04423\n",
      "Batch 2200/12000: train_loss = 0.04376\n",
      "Batch 2300/12000: train_loss = 0.04288\n",
      "Batch 2400/12000: train_loss = 0.04318\n",
      "Batch 2500/12000: train_loss = 0.04243\n",
      "Batch 2600/12000: train_loss = 0.04223\n",
      "Batch 2700/12000: train_loss = 0.04177\n",
      "Batch 2800/12000: train_loss = 0.04203\n",
      "Batch 2900/12000: train_loss = 0.04153\n",
      "Batch 3000/12000: train_loss = 0.04129\n",
      "Batch 3100/12000: train_loss = 0.04123\n",
      "Batch 3200/12000: train_loss = 0.04133\n",
      "Batch 3300/12000: train_loss = 0.04091\n",
      "Batch 3400/12000: train_loss = 0.04084\n",
      "Batch 3500/12000: train_loss = 0.04063\n",
      "Batch 3600/12000: train_loss = 0.04097\n",
      "Batch 3700/12000: train_loss = 0.04093\n",
      "Batch 3800/12000: train_loss = 0.04049\n",
      "Batch 3900/12000: train_loss = 0.04068\n",
      "Batch 4000/12000: train_loss = 0.04039\n",
      "Batch 4100/12000: train_loss = 0.04036\n",
      "Batch 4200/12000: train_loss = 0.04070\n",
      "Batch 4300/12000: train_loss = 0.04057\n",
      "Batch 4400/12000: train_loss = 0.04049\n",
      "Batch 4500/12000: train_loss = 0.04054\n",
      "Batch 4600/12000: train_loss = 0.04036\n",
      "Batch 4700/12000: train_loss = 0.04008\n",
      "Batch 4800/12000: train_loss = 0.04032\n",
      "Batch 4900/12000: train_loss = 0.03993\n",
      "Batch 5000/12000: train_loss = 0.03979\n",
      "Batch 5100/12000: train_loss = 0.03956\n",
      "Batch 5200/12000: train_loss = 0.03989\n",
      "Batch 5300/12000: train_loss = 0.03990\n",
      "Batch 5400/12000: train_loss = 0.04093\n",
      "Batch 5500/12000: train_loss = 0.04108\n",
      "Batch 5600/12000: train_loss = 0.04115\n",
      "Batch 5700/12000: train_loss = 0.04104\n",
      "Batch 5800/12000: train_loss = 0.04098\n",
      "Batch 5900/12000: train_loss = 0.04100\n",
      "Batch 6000/12000: train_loss = 0.04070\n",
      "Batch 6100/12000: train_loss = 0.04040\n",
      "Batch 6200/12000: train_loss = 0.04018\n",
      "Batch 6300/12000: train_loss = 0.04040\n",
      "Batch 6400/12000: train_loss = 0.04065\n",
      "Batch 6500/12000: train_loss = 0.04079\n",
      "Batch 6600/12000: train_loss = 0.04068\n",
      "Batch 6700/12000: train_loss = 0.04048\n",
      "Batch 6800/12000: train_loss = 0.04028\n",
      "Batch 6900/12000: train_loss = 0.04032\n",
      "Batch 7000/12000: train_loss = 0.04048\n",
      "Batch 7100/12000: train_loss = 0.04083\n",
      "Batch 7200/12000: train_loss = 0.04076\n",
      "Batch 7300/12000: train_loss = 0.04087\n",
      "Batch 7400/12000: train_loss = 0.04065\n",
      "Batch 7500/12000: train_loss = 0.04081\n",
      "Batch 7600/12000: train_loss = 0.04091\n",
      "Batch 7700/12000: train_loss = 0.04075\n",
      "Batch 7800/12000: train_loss = 0.04069\n",
      "Batch 7900/12000: train_loss = 0.04091\n",
      "Batch 8000/12000: train_loss = 0.04084\n",
      "Batch 8100/12000: train_loss = 0.04068\n",
      "Batch 8200/12000: train_loss = 0.04073\n",
      "Batch 8300/12000: train_loss = 0.04086\n",
      "Batch 8400/12000: train_loss = 0.04090\n",
      "Batch 8500/12000: train_loss = 0.04091\n",
      "Batch 8600/12000: train_loss = 0.04124\n",
      "Batch 8700/12000: train_loss = 0.04102\n",
      "Batch 8800/12000: train_loss = 0.04108\n",
      "Batch 8900/12000: train_loss = 0.04113\n",
      "Batch 9000/12000: train_loss = 0.04089\n",
      "Batch 9100/12000: train_loss = 0.04079\n",
      "Batch 9200/12000: train_loss = 0.04085\n",
      "Batch 9300/12000: train_loss = 0.04102\n",
      "Batch 9400/12000: train_loss = 0.04084\n",
      "Batch 9500/12000: train_loss = 0.04092\n",
      "Batch 9600/12000: train_loss = 0.04113\n",
      "Batch 9700/12000: train_loss = 0.04095\n",
      "Batch 9800/12000: train_loss = 0.04091\n",
      "Batch 9900/12000: train_loss = 0.04108\n",
      "Batch 10000/12000: train_loss = 0.04121\n",
      "Batch 10100/12000: train_loss = 0.04124\n",
      "Batch 10200/12000: train_loss = 0.04115\n",
      "Batch 10300/12000: train_loss = 0.04114\n",
      "Batch 10400/12000: train_loss = 0.04109\n",
      "Batch 10500/12000: train_loss = 0.04106\n",
      "Batch 10600/12000: train_loss = 0.04114\n",
      "Batch 10700/12000: train_loss = 0.04104\n",
      "Batch 10800/12000: train_loss = 0.04100\n",
      "Batch 10900/12000: train_loss = 0.04086\n",
      "Batch 11000/12000: train_loss = 0.04082\n",
      "Batch 11100/12000: train_loss = 0.04077\n",
      "Batch 11200/12000: train_loss = 0.04066\n",
      "Batch 11300/12000: train_loss = 0.04064\n",
      "Batch 11400/12000: train_loss = 0.04043\n",
      "Batch 11500/12000: train_loss = 0.04038\n",
      "Batch 11600/12000: train_loss = 0.04022\n",
      "Batch 11700/12000: train_loss = 0.04000\n",
      "Batch 11800/12000: train_loss = 0.03979\n",
      "Batch 11900/12000: train_loss = 0.03952\n",
      "Batch 12000/12000: train_loss = 0.03993\n",
      "Epoch 6 completed. Average training loss: 0.03993\n",
      "Epoch 7 is starting\n",
      "Batch 100/12000: train_loss = 0.04195\n",
      "Batch 200/12000: train_loss = 0.04618\n",
      "Batch 300/12000: train_loss = 0.04888\n",
      "Batch 400/12000: train_loss = 0.04622\n",
      "Batch 500/12000: train_loss = 0.04031\n",
      "Batch 600/12000: train_loss = 0.03883\n",
      "Batch 700/12000: train_loss = 0.03595\n",
      "Batch 800/12000: train_loss = 0.03479\n",
      "Batch 900/12000: train_loss = 0.03417\n",
      "Batch 1000/12000: train_loss = 0.03364\n",
      "Batch 1100/12000: train_loss = 0.03475\n",
      "Batch 1200/12000: train_loss = 0.03515\n",
      "Batch 1300/12000: train_loss = 0.03375\n",
      "Batch 1400/12000: train_loss = 0.03415\n",
      "Batch 1500/12000: train_loss = 0.03435\n",
      "Batch 1600/12000: train_loss = 0.03491\n",
      "Batch 1700/12000: train_loss = 0.03571\n",
      "Batch 1800/12000: train_loss = 0.03580\n",
      "Batch 1900/12000: train_loss = 0.03584\n",
      "Batch 2000/12000: train_loss = 0.03494\n",
      "Batch 2100/12000: train_loss = 0.03475\n",
      "Batch 2200/12000: train_loss = 0.03454\n",
      "Batch 2300/12000: train_loss = 0.03373\n",
      "Batch 2400/12000: train_loss = 0.03396\n",
      "Batch 2500/12000: train_loss = 0.03337\n",
      "Batch 2600/12000: train_loss = 0.03307\n",
      "Batch 2700/12000: train_loss = 0.03263\n",
      "Batch 2800/12000: train_loss = 0.03271\n",
      "Batch 2900/12000: train_loss = 0.03228\n",
      "Batch 3000/12000: train_loss = 0.03196\n",
      "Batch 3100/12000: train_loss = 0.03184\n",
      "Batch 3200/12000: train_loss = 0.03182\n",
      "Batch 3300/12000: train_loss = 0.03141\n",
      "Batch 3400/12000: train_loss = 0.03129\n",
      "Batch 3500/12000: train_loss = 0.03112\n",
      "Batch 3600/12000: train_loss = 0.03144\n",
      "Batch 3700/12000: train_loss = 0.03138\n",
      "Batch 3800/12000: train_loss = 0.03105\n",
      "Batch 3900/12000: train_loss = 0.03113\n",
      "Batch 4000/12000: train_loss = 0.03088\n",
      "Batch 4100/12000: train_loss = 0.03081\n",
      "Batch 4200/12000: train_loss = 0.03101\n",
      "Batch 4300/12000: train_loss = 0.03087\n",
      "Batch 4400/12000: train_loss = 0.03087\n",
      "Batch 4500/12000: train_loss = 0.03094\n",
      "Batch 4600/12000: train_loss = 0.03075\n",
      "Batch 4700/12000: train_loss = 0.03049\n",
      "Batch 4800/12000: train_loss = 0.03069\n",
      "Batch 4900/12000: train_loss = 0.03038\n",
      "Batch 5000/12000: train_loss = 0.03020\n",
      "Batch 5100/12000: train_loss = 0.03001\n",
      "Batch 5200/12000: train_loss = 0.03034\n",
      "Batch 5300/12000: train_loss = 0.03032\n",
      "Batch 5400/12000: train_loss = 0.03125\n",
      "Batch 5500/12000: train_loss = 0.03140\n",
      "Batch 5600/12000: train_loss = 0.03149\n",
      "Batch 5700/12000: train_loss = 0.03139\n",
      "Batch 5800/12000: train_loss = 0.03134\n",
      "Batch 5900/12000: train_loss = 0.03139\n",
      "Batch 6000/12000: train_loss = 0.03114\n",
      "Batch 6100/12000: train_loss = 0.03088\n",
      "Batch 6200/12000: train_loss = 0.03070\n",
      "Batch 6300/12000: train_loss = 0.03085\n",
      "Batch 6400/12000: train_loss = 0.03100\n",
      "Batch 6500/12000: train_loss = 0.03112\n",
      "Batch 6600/12000: train_loss = 0.03101\n",
      "Batch 6700/12000: train_loss = 0.03081\n",
      "Batch 6800/12000: train_loss = 0.03062\n",
      "Batch 6900/12000: train_loss = 0.03072\n",
      "Batch 7000/12000: train_loss = 0.03089\n",
      "Batch 7100/12000: train_loss = 0.03124\n",
      "Batch 7200/12000: train_loss = 0.03123\n",
      "Batch 7300/12000: train_loss = 0.03132\n",
      "Batch 7400/12000: train_loss = 0.03116\n",
      "Batch 7500/12000: train_loss = 0.03126\n",
      "Batch 7600/12000: train_loss = 0.03138\n",
      "Batch 7700/12000: train_loss = 0.03126\n",
      "Batch 7800/12000: train_loss = 0.03124\n",
      "Batch 7900/12000: train_loss = 0.03138\n",
      "Batch 8000/12000: train_loss = 0.03134\n",
      "Batch 8100/12000: train_loss = 0.03119\n",
      "Batch 8200/12000: train_loss = 0.03125\n",
      "Batch 8300/12000: train_loss = 0.03127\n",
      "Batch 8400/12000: train_loss = 0.03130\n",
      "Batch 8500/12000: train_loss = 0.03129\n",
      "Batch 8600/12000: train_loss = 0.03158\n",
      "Batch 8700/12000: train_loss = 0.03142\n",
      "Batch 8800/12000: train_loss = 0.03146\n",
      "Batch 8900/12000: train_loss = 0.03148\n",
      "Batch 9000/12000: train_loss = 0.03130\n",
      "Batch 9100/12000: train_loss = 0.03124\n",
      "Batch 9200/12000: train_loss = 0.03124\n",
      "Batch 9300/12000: train_loss = 0.03138\n",
      "Batch 9400/12000: train_loss = 0.03122\n",
      "Batch 9500/12000: train_loss = 0.03129\n",
      "Batch 9600/12000: train_loss = 0.03146\n",
      "Batch 9700/12000: train_loss = 0.03134\n",
      "Batch 9800/12000: train_loss = 0.03131\n",
      "Batch 9900/12000: train_loss = 0.03148\n",
      "Batch 10000/12000: train_loss = 0.03155\n",
      "Batch 10100/12000: train_loss = 0.03157\n",
      "Batch 10200/12000: train_loss = 0.03151\n",
      "Batch 10300/12000: train_loss = 0.03151\n",
      "Batch 10400/12000: train_loss = 0.03149\n",
      "Batch 10500/12000: train_loss = 0.03146\n",
      "Batch 10600/12000: train_loss = 0.03148\n",
      "Batch 10700/12000: train_loss = 0.03140\n",
      "Batch 10800/12000: train_loss = 0.03133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10900/12000: train_loss = 0.03120\n",
      "Batch 11000/12000: train_loss = 0.03116\n",
      "Batch 11100/12000: train_loss = 0.03113\n",
      "Batch 11200/12000: train_loss = 0.03107\n",
      "Batch 11300/12000: train_loss = 0.03104\n",
      "Batch 11400/12000: train_loss = 0.03090\n",
      "Batch 11500/12000: train_loss = 0.03085\n",
      "Batch 11600/12000: train_loss = 0.03073\n",
      "Batch 11700/12000: train_loss = 0.03057\n",
      "Batch 11800/12000: train_loss = 0.03041\n",
      "Batch 11900/12000: train_loss = 0.03020\n",
      "Batch 12000/12000: train_loss = 0.03062\n",
      "Epoch 7 completed. Average training loss: 0.03062\n",
      "Epoch 8 is starting\n",
      "Batch 100/12000: train_loss = 0.03541\n",
      "Batch 200/12000: train_loss = 0.03686\n",
      "Batch 300/12000: train_loss = 0.03821\n",
      "Batch 400/12000: train_loss = 0.03609\n",
      "Batch 500/12000: train_loss = 0.03161\n",
      "Batch 600/12000: train_loss = 0.03081\n",
      "Batch 700/12000: train_loss = 0.02838\n",
      "Batch 800/12000: train_loss = 0.02725\n",
      "Batch 900/12000: train_loss = 0.02692\n",
      "Batch 1000/12000: train_loss = 0.02648\n",
      "Batch 1100/12000: train_loss = 0.02745\n",
      "Batch 1200/12000: train_loss = 0.02766\n",
      "Batch 1300/12000: train_loss = 0.02649\n",
      "Batch 1400/12000: train_loss = 0.02676\n",
      "Batch 1500/12000: train_loss = 0.02701\n",
      "Batch 1600/12000: train_loss = 0.02727\n",
      "Batch 1700/12000: train_loss = 0.02789\n",
      "Batch 1800/12000: train_loss = 0.02781\n",
      "Batch 1900/12000: train_loss = 0.02778\n",
      "Batch 2000/12000: train_loss = 0.02709\n",
      "Batch 2100/12000: train_loss = 0.02685\n",
      "Batch 2200/12000: train_loss = 0.02685\n",
      "Batch 2300/12000: train_loss = 0.02616\n",
      "Batch 2400/12000: train_loss = 0.02631\n",
      "Batch 2500/12000: train_loss = 0.02585\n",
      "Batch 2600/12000: train_loss = 0.02550\n",
      "Batch 2700/12000: train_loss = 0.02513\n",
      "Batch 2800/12000: train_loss = 0.02506\n",
      "Batch 2900/12000: train_loss = 0.02466\n",
      "Batch 3000/12000: train_loss = 0.02433\n",
      "Batch 3100/12000: train_loss = 0.02419\n",
      "Batch 3200/12000: train_loss = 0.02409\n",
      "Batch 3300/12000: train_loss = 0.02376\n",
      "Batch 3400/12000: train_loss = 0.02358\n",
      "Batch 3500/12000: train_loss = 0.02349\n",
      "Batch 3600/12000: train_loss = 0.02364\n",
      "Batch 3700/12000: train_loss = 0.02356\n",
      "Batch 3800/12000: train_loss = 0.02330\n",
      "Batch 3900/12000: train_loss = 0.02339\n",
      "Batch 4000/12000: train_loss = 0.02320\n",
      "Batch 4100/12000: train_loss = 0.02319\n",
      "Batch 4200/12000: train_loss = 0.02330\n",
      "Batch 4300/12000: train_loss = 0.02321\n",
      "Batch 4400/12000: train_loss = 0.02328\n",
      "Batch 4500/12000: train_loss = 0.02338\n",
      "Batch 4600/12000: train_loss = 0.02322\n",
      "Batch 4700/12000: train_loss = 0.02300\n",
      "Batch 4800/12000: train_loss = 0.02315\n",
      "Batch 4900/12000: train_loss = 0.02291\n",
      "Batch 5000/12000: train_loss = 0.02275\n",
      "Batch 5100/12000: train_loss = 0.02261\n",
      "Batch 5200/12000: train_loss = 0.02293\n",
      "Batch 5300/12000: train_loss = 0.02288\n",
      "Batch 5400/12000: train_loss = 0.02367\n",
      "Batch 5500/12000: train_loss = 0.02378\n",
      "Batch 5600/12000: train_loss = 0.02390\n",
      "Batch 5700/12000: train_loss = 0.02382\n",
      "Batch 5800/12000: train_loss = 0.02377\n",
      "Batch 5900/12000: train_loss = 0.02384\n",
      "Batch 6000/12000: train_loss = 0.02367\n",
      "Batch 6100/12000: train_loss = 0.02347\n",
      "Batch 6200/12000: train_loss = 0.02333\n",
      "Batch 6300/12000: train_loss = 0.02340\n",
      "Batch 6400/12000: train_loss = 0.02349\n",
      "Batch 6500/12000: train_loss = 0.02352\n",
      "Batch 6600/12000: train_loss = 0.02343\n",
      "Batch 6700/12000: train_loss = 0.02328\n",
      "Batch 6800/12000: train_loss = 0.02310\n",
      "Batch 6900/12000: train_loss = 0.02325\n",
      "Batch 7000/12000: train_loss = 0.02341\n",
      "Batch 7100/12000: train_loss = 0.02370\n",
      "Batch 7200/12000: train_loss = 0.02371\n",
      "Batch 7300/12000: train_loss = 0.02379\n",
      "Batch 7400/12000: train_loss = 0.02368\n",
      "Batch 7500/12000: train_loss = 0.02374\n",
      "Batch 7600/12000: train_loss = 0.02387\n",
      "Batch 7700/12000: train_loss = 0.02377\n",
      "Batch 7800/12000: train_loss = 0.02377\n",
      "Batch 7900/12000: train_loss = 0.02385\n",
      "Batch 8000/12000: train_loss = 0.02383\n",
      "Batch 8100/12000: train_loss = 0.02370\n",
      "Batch 8200/12000: train_loss = 0.02375\n",
      "Batch 8300/12000: train_loss = 0.02373\n",
      "Batch 8400/12000: train_loss = 0.02375\n",
      "Batch 8500/12000: train_loss = 0.02374\n",
      "Batch 8600/12000: train_loss = 0.02398\n",
      "Batch 8700/12000: train_loss = 0.02389\n",
      "Batch 8800/12000: train_loss = 0.02393\n",
      "Batch 8900/12000: train_loss = 0.02392\n",
      "Batch 9000/12000: train_loss = 0.02378\n",
      "Batch 9100/12000: train_loss = 0.02376\n",
      "Batch 9200/12000: train_loss = 0.02375\n",
      "Batch 9300/12000: train_loss = 0.02383\n",
      "Batch 9400/12000: train_loss = 0.02371\n",
      "Batch 9500/12000: train_loss = 0.02378\n",
      "Batch 9600/12000: train_loss = 0.02390\n",
      "Batch 9700/12000: train_loss = 0.02381\n",
      "Batch 9800/12000: train_loss = 0.02378\n",
      "Batch 9900/12000: train_loss = 0.02390\n",
      "Batch 10000/12000: train_loss = 0.02393\n",
      "Batch 10100/12000: train_loss = 0.02393\n",
      "Batch 10200/12000: train_loss = 0.02389\n",
      "Batch 10300/12000: train_loss = 0.02387\n",
      "Batch 10400/12000: train_loss = 0.02385\n",
      "Batch 10500/12000: train_loss = 0.02382\n",
      "Batch 10600/12000: train_loss = 0.02379\n",
      "Batch 10700/12000: train_loss = 0.02373\n",
      "Batch 10800/12000: train_loss = 0.02364\n",
      "Batch 10900/12000: train_loss = 0.02354\n",
      "Batch 11000/12000: train_loss = 0.02350\n",
      "Batch 11100/12000: train_loss = 0.02349\n",
      "Batch 11200/12000: train_loss = 0.02345\n",
      "Batch 11300/12000: train_loss = 0.02342\n",
      "Batch 11400/12000: train_loss = 0.02332\n",
      "Batch 11500/12000: train_loss = 0.02329\n",
      "Batch 11600/12000: train_loss = 0.02319\n",
      "Batch 11700/12000: train_loss = 0.02307\n",
      "Batch 11800/12000: train_loss = 0.02296\n",
      "Batch 11900/12000: train_loss = 0.02280\n",
      "Batch 12000/12000: train_loss = 0.02320\n",
      "Epoch 8 completed. Average training loss: 0.02320\n",
      "Epoch 9 is starting\n",
      "Batch 100/12000: train_loss = 0.02935\n",
      "Batch 200/12000: train_loss = 0.02883\n",
      "Batch 300/12000: train_loss = 0.02835\n",
      "Batch 400/12000: train_loss = 0.02677\n",
      "Batch 500/12000: train_loss = 0.02373\n",
      "Batch 600/12000: train_loss = 0.02342\n",
      "Batch 700/12000: train_loss = 0.02149\n",
      "Batch 800/12000: train_loss = 0.02059\n",
      "Batch 900/12000: train_loss = 0.02059\n",
      "Batch 1000/12000: train_loss = 0.02040\n",
      "Batch 1100/12000: train_loss = 0.02109\n",
      "Batch 1200/12000: train_loss = 0.02109\n",
      "Batch 1300/12000: train_loss = 0.02017\n",
      "Batch 1400/12000: train_loss = 0.02045\n",
      "Batch 1500/12000: train_loss = 0.02070\n",
      "Batch 1600/12000: train_loss = 0.02079\n",
      "Batch 1700/12000: train_loss = 0.02131\n",
      "Batch 1800/12000: train_loss = 0.02116\n",
      "Batch 1900/12000: train_loss = 0.02108\n",
      "Batch 2000/12000: train_loss = 0.02056\n",
      "Batch 2100/12000: train_loss = 0.02034\n",
      "Batch 2200/12000: train_loss = 0.02046\n",
      "Batch 2300/12000: train_loss = 0.01992\n",
      "Batch 2400/12000: train_loss = 0.01994\n",
      "Batch 2500/12000: train_loss = 0.01957\n",
      "Batch 2600/12000: train_loss = 0.01934\n",
      "Batch 2700/12000: train_loss = 0.01903\n",
      "Batch 2800/12000: train_loss = 0.01889\n",
      "Batch 2900/12000: train_loss = 0.01858\n",
      "Batch 3000/12000: train_loss = 0.01828\n",
      "Batch 3100/12000: train_loss = 0.01814\n",
      "Batch 3200/12000: train_loss = 0.01803\n",
      "Batch 3300/12000: train_loss = 0.01778\n",
      "Batch 3400/12000: train_loss = 0.01766\n",
      "Batch 3500/12000: train_loss = 0.01759\n",
      "Batch 3600/12000: train_loss = 0.01764\n",
      "Batch 3700/12000: train_loss = 0.01753\n",
      "Batch 3800/12000: train_loss = 0.01733\n",
      "Batch 3900/12000: train_loss = 0.01743\n",
      "Batch 4000/12000: train_loss = 0.01729\n",
      "Batch 4100/12000: train_loss = 0.01728\n",
      "Batch 4200/12000: train_loss = 0.01732\n",
      "Batch 4300/12000: train_loss = 0.01722\n",
      "Batch 4400/12000: train_loss = 0.01731\n",
      "Batch 4500/12000: train_loss = 0.01739\n",
      "Batch 4600/12000: train_loss = 0.01724\n",
      "Batch 4700/12000: train_loss = 0.01706\n",
      "Batch 4800/12000: train_loss = 0.01715\n",
      "Batch 4900/12000: train_loss = 0.01698\n",
      "Batch 5000/12000: train_loss = 0.01687\n",
      "Batch 5100/12000: train_loss = 0.01678\n",
      "Batch 5200/12000: train_loss = 0.01707\n",
      "Batch 5300/12000: train_loss = 0.01701\n",
      "Batch 5400/12000: train_loss = 0.01761\n",
      "Batch 5500/12000: train_loss = 0.01771\n",
      "Batch 5600/12000: train_loss = 0.01782\n",
      "Batch 5700/12000: train_loss = 0.01776\n",
      "Batch 5800/12000: train_loss = 0.01767\n",
      "Batch 5900/12000: train_loss = 0.01774\n",
      "Batch 6000/12000: train_loss = 0.01762\n",
      "Batch 6100/12000: train_loss = 0.01746\n",
      "Batch 6200/12000: train_loss = 0.01736\n",
      "Batch 6300/12000: train_loss = 0.01739\n",
      "Batch 6400/12000: train_loss = 0.01746\n",
      "Batch 6500/12000: train_loss = 0.01747\n",
      "Batch 6600/12000: train_loss = 0.01741\n",
      "Batch 6700/12000: train_loss = 0.01730\n",
      "Batch 6800/12000: train_loss = 0.01716\n",
      "Batch 6900/12000: train_loss = 0.01732\n",
      "Batch 7000/12000: train_loss = 0.01749\n",
      "Batch 7100/12000: train_loss = 0.01769\n",
      "Batch 7200/12000: train_loss = 0.01774\n",
      "Batch 7300/12000: train_loss = 0.01783\n",
      "Batch 7400/12000: train_loss = 0.01775\n",
      "Batch 7500/12000: train_loss = 0.01777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7600/12000: train_loss = 0.01790\n",
      "Batch 7700/12000: train_loss = 0.01781\n",
      "Batch 7800/12000: train_loss = 0.01783\n",
      "Batch 7900/12000: train_loss = 0.01788\n",
      "Batch 8000/12000: train_loss = 0.01790\n",
      "Batch 8100/12000: train_loss = 0.01778\n",
      "Batch 8200/12000: train_loss = 0.01779\n",
      "Batch 8300/12000: train_loss = 0.01776\n",
      "Batch 8400/12000: train_loss = 0.01778\n",
      "Batch 8500/12000: train_loss = 0.01774\n",
      "Batch 8600/12000: train_loss = 0.01790\n",
      "Batch 8700/12000: train_loss = 0.01788\n",
      "Batch 8800/12000: train_loss = 0.01790\n",
      "Batch 8900/12000: train_loss = 0.01788\n",
      "Batch 9000/12000: train_loss = 0.01777\n",
      "Batch 9100/12000: train_loss = 0.01778\n",
      "Batch 9200/12000: train_loss = 0.01778\n",
      "Batch 9300/12000: train_loss = 0.01786\n",
      "Batch 9400/12000: train_loss = 0.01777\n",
      "Batch 9500/12000: train_loss = 0.01784\n",
      "Batch 9600/12000: train_loss = 0.01786\n",
      "Batch 9700/12000: train_loss = 0.01781\n",
      "Batch 9800/12000: train_loss = 0.01778\n",
      "Batch 9900/12000: train_loss = 0.01788\n",
      "Batch 10000/12000: train_loss = 0.01790\n",
      "Batch 10100/12000: train_loss = 0.01789\n",
      "Batch 10200/12000: train_loss = 0.01785\n",
      "Batch 10300/12000: train_loss = 0.01785\n",
      "Batch 10400/12000: train_loss = 0.01783\n",
      "Batch 10500/12000: train_loss = 0.01782\n",
      "Batch 10600/12000: train_loss = 0.01778\n",
      "Batch 10700/12000: train_loss = 0.01772\n",
      "Batch 10800/12000: train_loss = 0.01766\n",
      "Batch 10900/12000: train_loss = 0.01757\n",
      "Batch 11000/12000: train_loss = 0.01753\n",
      "Batch 11100/12000: train_loss = 0.01754\n",
      "Batch 11200/12000: train_loss = 0.01748\n",
      "Batch 11300/12000: train_loss = 0.01744\n",
      "Batch 11400/12000: train_loss = 0.01738\n",
      "Batch 11500/12000: train_loss = 0.01735\n",
      "Batch 11600/12000: train_loss = 0.01727\n",
      "Batch 11700/12000: train_loss = 0.01720\n",
      "Batch 11800/12000: train_loss = 0.01711\n",
      "Batch 11900/12000: train_loss = 0.01700\n",
      "Batch 12000/12000: train_loss = 0.01736\n",
      "Epoch 9 completed. Average training loss: 0.01736\n",
      "Epoch 10 is starting\n",
      "Batch 100/12000: train_loss = 0.02331\n",
      "Batch 200/12000: train_loss = 0.02152\n",
      "Batch 300/12000: train_loss = 0.02054\n",
      "Batch 400/12000: train_loss = 0.01971\n",
      "Batch 500/12000: train_loss = 0.01753\n",
      "Batch 600/12000: train_loss = 0.01743\n",
      "Batch 700/12000: train_loss = 0.01585\n",
      "Batch 800/12000: train_loss = 0.01519\n",
      "Batch 900/12000: train_loss = 0.01539\n",
      "Batch 1000/12000: train_loss = 0.01518\n",
      "Batch 1100/12000: train_loss = 0.01560\n",
      "Batch 1200/12000: train_loss = 0.01550\n",
      "Batch 1300/12000: train_loss = 0.01483\n",
      "Batch 1400/12000: train_loss = 0.01522\n",
      "Batch 1500/12000: train_loss = 0.01555\n",
      "Batch 1600/12000: train_loss = 0.01562\n",
      "Batch 1700/12000: train_loss = 0.01619\n",
      "Batch 1800/12000: train_loss = 0.01608\n",
      "Batch 1900/12000: train_loss = 0.01593\n",
      "Batch 2000/12000: train_loss = 0.01555\n",
      "Batch 2100/12000: train_loss = 0.01536\n",
      "Batch 2200/12000: train_loss = 0.01551\n",
      "Batch 2300/12000: train_loss = 0.01512\n",
      "Batch 2400/12000: train_loss = 0.01508\n",
      "Batch 2500/12000: train_loss = 0.01479\n",
      "Batch 2600/12000: train_loss = 0.01458\n",
      "Batch 2700/12000: train_loss = 0.01432\n",
      "Batch 2800/12000: train_loss = 0.01417\n",
      "Batch 2900/12000: train_loss = 0.01395\n",
      "Batch 3000/12000: train_loss = 0.01372\n",
      "Batch 3100/12000: train_loss = 0.01363\n",
      "Batch 3200/12000: train_loss = 0.01355\n",
      "Batch 3300/12000: train_loss = 0.01337\n",
      "Batch 3400/12000: train_loss = 0.01332\n",
      "Batch 3500/12000: train_loss = 0.01328\n",
      "Batch 3600/12000: train_loss = 0.01329\n",
      "Batch 3700/12000: train_loss = 0.01318\n",
      "Batch 3800/12000: train_loss = 0.01302\n",
      "Batch 3900/12000: train_loss = 0.01310\n",
      "Batch 4000/12000: train_loss = 0.01302\n",
      "Batch 4100/12000: train_loss = 0.01303\n",
      "Batch 4200/12000: train_loss = 0.01304\n",
      "Batch 4300/12000: train_loss = 0.01298\n",
      "Batch 4400/12000: train_loss = 0.01309\n",
      "Batch 4500/12000: train_loss = 0.01310\n",
      "Batch 4600/12000: train_loss = 0.01300\n",
      "Batch 4700/12000: train_loss = 0.01284\n",
      "Batch 4800/12000: train_loss = 0.01291\n",
      "Batch 4900/12000: train_loss = 0.01279\n",
      "Batch 5000/12000: train_loss = 0.01271\n",
      "Batch 5100/12000: train_loss = 0.01265\n",
      "Batch 5200/12000: train_loss = 0.01292\n",
      "Batch 5300/12000: train_loss = 0.01289\n",
      "Batch 5400/12000: train_loss = 0.01334\n",
      "Batch 5500/12000: train_loss = 0.01342\n",
      "Batch 5600/12000: train_loss = 0.01353\n",
      "Batch 5700/12000: train_loss = 0.01347\n",
      "Batch 5800/12000: train_loss = 0.01339\n",
      "Batch 5900/12000: train_loss = 0.01346\n",
      "Batch 6000/12000: train_loss = 0.01337\n",
      "Batch 6100/12000: train_loss = 0.01326\n",
      "Batch 6200/12000: train_loss = 0.01318\n",
      "Batch 6300/12000: train_loss = 0.01318\n",
      "Batch 6400/12000: train_loss = 0.01322\n",
      "Batch 6500/12000: train_loss = 0.01325\n",
      "Batch 6600/12000: train_loss = 0.01320\n",
      "Batch 6700/12000: train_loss = 0.01310\n",
      "Batch 6800/12000: train_loss = 0.01300\n",
      "Batch 6900/12000: train_loss = 0.01317\n",
      "Batch 7000/12000: train_loss = 0.01329\n",
      "Batch 7100/12000: train_loss = 0.01344\n",
      "Batch 7200/12000: train_loss = 0.01347\n",
      "Batch 7300/12000: train_loss = 0.01356\n",
      "Batch 7400/12000: train_loss = 0.01349\n",
      "Batch 7500/12000: train_loss = 0.01349\n",
      "Batch 7600/12000: train_loss = 0.01361\n",
      "Batch 7700/12000: train_loss = 0.01355\n",
      "Batch 7800/12000: train_loss = 0.01357\n",
      "Batch 7900/12000: train_loss = 0.01361\n",
      "Batch 8000/12000: train_loss = 0.01363\n",
      "Batch 8100/12000: train_loss = 0.01355\n",
      "Batch 8200/12000: train_loss = 0.01352\n",
      "Batch 8300/12000: train_loss = 0.01351\n",
      "Batch 8400/12000: train_loss = 0.01352\n",
      "Batch 8500/12000: train_loss = 0.01347\n",
      "Batch 8600/12000: train_loss = 0.01358\n",
      "Batch 8700/12000: train_loss = 0.01357\n",
      "Batch 8800/12000: train_loss = 0.01358\n",
      "Batch 8900/12000: train_loss = 0.01355\n",
      "Batch 9000/12000: train_loss = 0.01347\n",
      "Batch 9100/12000: train_loss = 0.01348\n",
      "Batch 9200/12000: train_loss = 0.01348\n",
      "Batch 9300/12000: train_loss = 0.01356\n",
      "Batch 9400/12000: train_loss = 0.01349\n",
      "Batch 9500/12000: train_loss = 0.01353\n",
      "Batch 9600/12000: train_loss = 0.01351\n",
      "Batch 9700/12000: train_loss = 0.01347\n",
      "Batch 9800/12000: train_loss = 0.01343\n",
      "Batch 9900/12000: train_loss = 0.01349\n",
      "Batch 10000/12000: train_loss = 0.01349\n",
      "Batch 10100/12000: train_loss = 0.01347\n",
      "Batch 10200/12000: train_loss = 0.01343\n",
      "Batch 10300/12000: train_loss = 0.01342\n",
      "Batch 10400/12000: train_loss = 0.01342\n",
      "Batch 10500/12000: train_loss = 0.01340\n",
      "Batch 10600/12000: train_loss = 0.01337\n",
      "Batch 10700/12000: train_loss = 0.01332\n",
      "Batch 10800/12000: train_loss = 0.01327\n",
      "Batch 10900/12000: train_loss = 0.01321\n",
      "Batch 11000/12000: train_loss = 0.01319\n",
      "Batch 11100/12000: train_loss = 0.01320\n",
      "Batch 11200/12000: train_loss = 0.01316\n",
      "Batch 11300/12000: train_loss = 0.01311\n",
      "Batch 11400/12000: train_loss = 0.01307\n",
      "Batch 11500/12000: train_loss = 0.01302\n",
      "Batch 11600/12000: train_loss = 0.01297\n",
      "Batch 11700/12000: train_loss = 0.01292\n",
      "Batch 11800/12000: train_loss = 0.01286\n",
      "Batch 11900/12000: train_loss = 0.01277\n",
      "Batch 12000/12000: train_loss = 0.01308\n",
      "Epoch 10 completed. Average training loss: 0.01308\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "n_epochs = 10\n",
    "#model.to(device)   \n",
    "model.train()\n",
    "t_loss = []\n",
    "#data = data.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} is starting')\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for i, (data, target) in enumerate(train_dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Batch {i}/{len(train_dataloader)}: train_loss = {train_loss / (i * data.size(0)):.5f}')\n",
    "    \n",
    "    # Calculate the average training loss for the epoch\n",
    "    train_loss /= len(training_data)\n",
    "    t_loss.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} completed. Average training loss: {train_loss:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f121f7b44f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABvCAYAAAD8BTu/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa+ElEQVR4nO3de3xU9Zn48c8zl0yukBBASoIEFEFEDBrwVltoraJ4+bWKl2IXsG7VqnRZLbZubVmX/sRtf7V1t9Vailh1Ubx1sVrtalVcaQsBREVhV7kFkHsSEsIkM5Pn98c5CZMwk0xC4CST5/16zWvO5XvOeeY7yXO+cy7fI6qKMcaYns/ndQDGGGO6hiV0Y4xJE5bQjTEmTVhCN8aYNGEJ3Rhj0oQldGOMSROW0I1nROQtEbnJ6zhaE5FFIjKvk8t+VUQqRKRWRMZ1dWxJttnpeI8HEZkhIv/tdRy9gSX0HkZENovIhV7HEU9ESkRERSTQRpm5IvLk8YzLIz8FblfVXFVd43UwpnexhG5M1xoKrPM6CNM7WUJPEyISEpGfi8gO9/VzEQm58/qLyB9EpEpE9ovIOyLic+fdLSLbRaRGRDaIyJeTrH+KiKwRkQPuIYW5cbOXue9V7qGGc1stOxm4B7jWnb82bvZQEXnX3f6fRKR/3HLniMhyN+61IjKxjc8/WESeF5E9IrJJRGbFzZsrIktE5HfudtaJSFnc/HEistqd9wyQ2cZ2fCLyAxHZIiK73XX2deu/FvADa0Xk0yTLjxKR/3K/hw0ick2KdYyIfD6uPipEZEbc7AIRedn9DH8TkZPa+AxJ69U9DHa/iKxw4/hPEekXN/8Kt/6q3LKnxs0bIiIvuN/BPhH591bb/amIVLrfzyXJ4jNHQVXt1YNewGbgwgTT7wP+CgwEBgDLgX9x590PPAIE3dcFgAAjgQpgsFuuBDgpyXYnAqfjNALGAruA/xO3nAKBNuKeCzzZatpbwKfAKUCWOz7fnVcE7AMudbf5FXd8QIJ1+4BVwA+BDGA4sBG4OG7bYXddfrc+/urOywC2ALPdurkaiADzknyOG4FP3G3kAi8AT8TNV+DkJMvmuPU9EwgA44C9wOgU6ngoUANc78ZZCJS68xa5dTPBXe9TwNNJYmizXt3vYDswxo33+abvzf2eDrrLBIE5bl1kuPW6FnjQXS4T+Ly73Ay3Tv/eLXcrsAMQr/+f0u3leQD26uAXljyhfwpcGjd+MbDZHb4P+M/WiQY4GdgNXAgEOxjHz4EH3eESOp/QfxA3/m3gVXf47vhE6U57DZieYN1nA1tbTfs+8Fjctl+PmzcaOOQOf6F1csHZGSZL6G8A344bH+kmq4A73lZCvxZ4p9W0XwM/SqGOvw+8mKTcImBB3PilwPokZdusV+J2qnF11eAm4nuBJXHzfDjJfyJwLrAn0d+Am9A/iRvPdutp0LH8X+mNLzvkkj4G47Q0m2xxpwH8BKcl9ScR2Sgi3wNQ1U+Af8BJeLtF5GkRGUwCInK2iLzp/pyuBm4B+icq20E744brcFq94LRIp7o/7atEpAr4PPC5BOsYCgxuVfYe4IQ2tpPpnsQdDGxXN9O44uuxtUT1HGi1rWSGAme3inMaMAjareMhODvtZJLVY6IY2qvXilafL+jG0eKzq2qjW7bIjW+Lqkbbi09V69zBZDGaTrKEnj524PyzNjnRnYaq1qjqnao6HLgC+MemY+Wq+h+q+nl3WQUeSLL+/wCWAkNUtS/OIRxx56XSZWdHu/WswGlJ5se9clR1fpKym1qVzVPVS1PYzmdAkYhI3LQT2yifqJ6jOIdH2lMBvN0qzlxVvdWd31YdVwBJj4t3QCr1OiRu+EScXyB7afXZ3TobgtNKrwBOlDaudDLHniX0nikoIplxrwCwGPiBiAxwTyz+EHgSQEQuE5GT3X/AaiAGNIrISBH5kjgnT8PAIaAxyTbzgP2qGhaRCcDX4+btcZcb3kbMu4AScU/GpuBJ4HIRuVhE/O7nnCgixQnKrgBqxDnBm+WWHyMi41PYzl9wEvIsEQmKyNdwjkUnsxiYLSLDRCQX+L/AM220TOP9AThFRL7hbisoIuPjTiy2VcdPAReKyDUiEhCRQhEpTWGbraVSrzeIyGgRycY5XPecqsaAJcAUEfmyiASBO4F6nENUK3B2jvNFJMdd7/mdiM8cBUvoPdMrOMm36TUXmAeUA+8DHwCr3WkAI4DXgVqcBPYrVX0TCAHzcVpfO3FOqH4/yTa/DdwnIjU4O4slTTPcn9A/Bt51f8afk2D5Z933fSKyur0PqKoVwJU4h0724LQAv0uCv1k32VwGlAKb3M+zAOibwnYagK/hHOfdj3Oc+4U2FlkIPIFzZc8mnB3hHe1tx91WDXARcB1Oa3cnzi+ikFukrTreinNs/E43zveAM1LZbqsYUqnXJ3COy+/EObk5y112A3AD8G84dXw5cLmqNrjfweU452W2Attw6tIcR9Ly0KExpjcTkbdwTl4v8DoW03HWQjfGmDRhCd0YY9KEHXIxxpg0kVILXUQmi3Ob8idN1zAnKXeVOJ00lSUrY4wx5thoN6GLiB/4JXAJzl1j14vI6ATl8oDvAH/r6iCNMca0L5WbACbg3La7EUBEnsa57OmjVuX+BecSrO+msuH+/ftrSUlJ6pEaY4xh1apVe1V1QKJ5qST0IlreCrwNp++MZiJyJs7dbS+LSEoJvaSkhPLy8lSKHmF3TZiBeUk7xDPGmLQlIkm7pjjqq1zcO/9+hnPDQ3tlvyUi5SJSvmfPnk5t79dvf8rFDy7j0z21nVreGGPSVSoJfTst+3Yodqc1ycPpavMtEdkMnAMsTXRiVFUfVdUyVS0bMCDhL4Z2TR4zCJ8I0xeuYHdNuFPrMMaYdJRKQl8JjHD7rsjAuW15adNMVa1W1f6qWqKqJTh9cl+hqp07ntKOoYU5PDZzPPsPNjDzsZXU1qfShYYxxqS/do+hq2pURG7H6TPZDyxU1XUich9QrqpL215D1xtbnM8vp53JTY+Xc+uTq/jt9PFkBOweKWOORiQSYdu2bYTD9su3O8jMzKS4uJhgMJjyMp7dWFRWVqadPSnaZEl5BXOee5+vjiviZ9ecQcseUI0xHbFp0yby8vIoLCy0/yWPqSr79u2jpqaGYcOGtZgnIqtUNeG9Pj26WXtN2RDu/MopvLhmO//62gavwzGmRwuHw5bMuwkRobCwsMO/lnp8Z/S3f+lkdh4I8/BbnzKoTybTzyvxOiRjeixL5t1HZ76LHt1CB+dD33flGL4y+gTmvrSOVz/8zOuQjDGdsG/fPkpLSyktLWXQoEEUFRU1jzc0NLS5bHl5ObNmzWp3G+edd16XxPrWW29x2WWXdcm6ulKPb6ED+H3CQ9eNY9qCvzLr6fd46qYQ40v6eR2WMaYDCgsLee+99wCYO3cuubm53HXXXc3zo9EogUDilFVWVkZZWftdSC1fvrxLYu2uenwLvUlWhp/fTh9PcUEWNz1ezv/uqvE6JGPMUZoxYwa33HILZ599NnPmzGHFihWce+65jBs3jvPOO48NG5xzZ/Et5rlz53LjjTcyceJEhg8fzkMPPdS8vtzc3ObyEydO5Oqrr2bUqFFMmzaNpgtEXnnlFUaNGsVZZ53FrFmzOtQSX7x4Maeffjpjxozh7rvvBiAWizFjxgzGjBnD6aefzoMPPgjAQw89xOjRoxk7dizXXXfd0VcWadJCb1KQk8HjMyfwtYeXM33hCl749vkM6mtdBBjTUf/80jo+2nGgS9c5enAffnT5aR1ebtu2bSxfvhy/38+BAwd45513CAQCvP7669xzzz08//zzRyyzfv163nzzTWpqahg5ciS33nrrEZf/rVmzhnXr1jF48GDOP/983n33XcrKyrj55ptZtmwZw4YN4/rrr085zh07dnD33XezatUqCgoKuOiii/j973/PkCFD2L59Ox9++CEAVVVVAMyfP59NmzYRCoWapx2ttGmhNxnSL5vHZoyn+lCEGY+t4EA44nVIxpijMHXqVPx+PwDV1dVMnTqVMWPGMHv2bNatW5dwmSlTphAKhejfvz8DBw5k165dR5SZMGECxcXF+Hw+SktL2bx5M+vXr2f48OHNlwp2JKGvXLmSiRMnMmDAAAKBANOmTWPZsmUMHz6cjRs3cscdd/Dqq6/Sp08fAMaOHcu0adN48sknkx5K6qi0aqE3GVPUl0e+cRYzH1vJzb9bxaIbxxMK+L0Oy5geozMt6WMlJyenefjee+9l0qRJvPjii2zevJmJEycmXCYUCjUP+/1+otEj7yhPpUxXKCgoYO3atbz22ms88sgjLFmyhIULF/Lyyy+zbNkyXnrpJX784x/zwQcfHHViT7sWepMLRgzgJ1PH8peN+7jr2fdpbLQnMxnT01VXV1NUVATAokWLunz9I0eOZOPGjWzevBmAZ555JuVlJ0yYwNtvv83evXuJxWIsXryYL37xi+zdu5fGxkauuuoq5s2bx+rVq2lsbKSiooJJkybxwAMPUF1dTW3t0Xc4mJYt9CZfHVfMrgP1zP/jegb1CfFPU454LocxpgeZM2cO06dPZ968eUyZMqXL15+VlcWvfvUrJk+eTE5ODuPHj09a9o033qC4uLh5/Nlnn2X+/PlMmjQJVWXKlClceeWVrF27lpkzZ9LY2AjA/fffTywW44YbbqC6uhpVZdasWeTn5x91/D361v9UqCr//NJHLFq+mR9MOZWbLhh+zLdpTE/08ccfc+qpp3odhudqa2vJzc1FVbntttsYMWIEs2fP9iSWRN9J2t76nwoR4d7LRnPJmEHMe/ljXlq7w+uQjDHd2G9+8xtKS0s57bTTqK6u5uabb/Y6pJSl9SGXJn6f8OC1peyrXcGdS9bSPzfEuScVeh2WMaYbmj17tmct8qOV9i30JplBP7/5uzKGFmbzrSfKWb+za6+xNcYYr/WahA7QNzvIohsnkJ3hZ/rCFWyvOuR1SMZ0K16dUzNH6sx30asSOkBRfhaP3ziBuvoYMxauoLrObjwyBpwHKuzbt8+SejfQ1B96ZmbH7nRP+6tckln+6V5mLFxJ6ZB8fvfNCWQG7cYj07vZE4u6l2RPLGrrKpdem9ABXlq7gzsWr+HS0wfxb9efid9nfUEbY7q3thJ6r7jKJZnLzxjMrgNh5r38MQPzPuJHl4+2Dv6NMT1Wr07oADddMJyd1WEW/PcmPtc3k5u/eJLXIRljTKf0+oQOcM+lp7LzQJj7/7iegX1CfHVccfsLGWNMN2MJHfD5hP93zRnsra3nu8++T//cEBeMGOB1WMYY0yEpXbYoIpNFZIOIfCIi30sw/x9F5CMReV9E3hCRoV0f6rEVCvh59O/KOHlgLrc8sYoPt1d7HZIxxnRIuwldRPzAL4FLgNHA9SLSutvCNUCZqo4FngP+tasDPR76ZAZZNHMCfbOCzFy0kor9dV6HZIwxKUulhT4B+ERVN6pqA/A0cGV8AVV9U1Wbst9fgR57EHpQ30wev3EC9ZEY0x9bQeXBtp82bowx3UUqCb0IqIgb3+ZOS+abwB8TzRCRb4lIuYiU79mzJ/Uoj7MRJ+SxYPp4tlUe4puPryQciXkdkjHGtKtLb/0XkRuAMuAniear6qOqWqaqZQMGdO+TjhOG9eMX15aypqKKOxavIWZPPDLGdHOpJPTtwJC48WJ3WgsiciHwT8AVqlrfNeF565LTP8fcy0/jvz7axY+Wfmh9XBhjurVULltcCYwQkWE4ifw64OvxBURkHPBrYLKq7u7yKD00/bwSPqsO88jbnzKoTya3f2mE1yEZY0xC7SZ0VY2KyO3Aa4AfWKiq60TkPqBcVZfiHGLJBZ51b53fqqpXHMO4j6s5F49k14EwP/3T/3BCn0ymlg1pfyFjjDnOUrqxSFVfAV5pNe2HccMXdnFc3YrPJzxw1Vj21tbzvRc+oH9eiEkjB3odljHGtNDr+kPvrIyAj4dvOItRg/K47anVvL+tyuuQjDGmBUvoHZAbCvDYzPH0y8ngxkUr2bLvoNchGWNMs17dH3pnfbqnlqseXs6hhhhnDMmnbGgBZ7mv/OwMr8MzxqQx6w+9i500IJclN5/L0ysqWLVlP48u20jUvU79pAE5lA3t5yT4kgKG98+xPtaNMceFtdC7wKGGGGu3VbFqS2Xzq/qQ86zSguwgZw0t4MyhBZQN7cfY4r72uDtjTKdZC/0Yy8rwc87wQs4ZXghAY6OycW8tq7ZUUr65klVbK3n9Y+fy/KBfOG1wX84aWtB8qGZgn449CNYYYxKxFvpxsv9gQ1wLfj9rt1XTEG0EYEi/LMqG9nNb8QWcckKePd/UGJOQPSS6G2qINvLhjmpWu6348i2V7K11ekzICwUoPTHfbcX3o/TEfHJD9mPKGGMJvUdQVSr2H2LV1v3OYZotlWzYVYMq+ARGDerjJPiSAs48sYDigiw72WpML2QJvYc6EI7w3tYqyrdUsnpLJWu2VnKwwenK94Q+IfdSyX6cOiiPwflZDOqbaSdcjUlzdlK0h+qTGeQLpwzgC6c4XQ1HY41s2FXTfCy+fHMlr3yws8Uy/XNDFOVnUlSQxeC+WQzOd15F+VkMzs+kX06GteyNSVPWQu/hdlaH2bT3INurDrHDfW2Pew9HGluUzwz6Dif45oSfSVF+FkUFTis/FLBWvjHdlbXQ09igvpkM6pv4skdVpaouwva4JO+8wmyvOsSbO3ezu+bIrusH5IXcpJ/ZnPSLCppa+VkUZAetlW9MN2QJPY2JCAU5GRTkZDCmqG/CMvXRGDurw27CD7do5a/fWcOf1+9ut5VfVOAk+sLcDAqyM+iXnUF+TpC8UMASvzHHkSX0Xi4U8DO0MIehhTkJ56sqlXWRlodyKg+xo/oQ26vCrN+5mz0JWvkAAZ+Qnx0kvynJZwcpcJN9Qfy0nAwK3HL5WUECfuszzpjOsIRu2iQi9MvJoF8Krfy9tQ1U1TVQWRehqq6B/QcPD1fWNbB1fx3vVVRRVRehIdaYcF0AfTIDFORkuDsCdyeQ7Sb9HGdH0LQDKHB3DnZ1jzGW0E0XaK+V35qqUtcQo7Kugaq6iJv4neHKugYq3R1BZV0De2sb+N/dtVQebGi+ZDORrKC/OcnnZQbIywyQGwqQmxkgNxQkN+R3x4PkhlrOz3Pfs4J+O0RkejRL6Oa4ExFyQgFyQgGKC1Jfrj4ao7ou0pzs4xN//C+DmnCUz6rD1NZHqQ1HqamPNnez0Baf4Cb7YNzOIO7ljrfcWTSNB1uUt64bjBcsoZseIxTwM7CPv1OdmdVHYxysj7kJPkJtOOok/PooNU3D4cPjB915VYcibKusa57f1q+EeFlBP9kZfjLd9+wMP1kZfrIznF8CWXHTmspmZQTIdudlZfjJDrrl48azMvyEAj77JWESsoRueoVQwE8o4KdfztE9gCTWqBxsaJn8D+8MIi3GD0ViHGqIUdcQoy4SI9wQY3dNmLoGZ/qhiDMvlV8P8XyCu1MINO8sWu44AmQFfWRnBMgM+skM+sgMOjuC+PfMoI9Q4Mj3UFz5DL/tPHoSS+jGdIDfJ/TJDNInM9hl64zGGp3kH78DaIgRjjQNR1vsAJrKHIpE44ad96q6iDscbV5HJNb5mwdFOHJH0JT03fdEO4X498ygnwx355AR8BH0+5ydReDweNO8UNN403y/j6BfbKeSopQSuohMBn4B+IEFqjq/1fwQ8DvgLGAfcK2qbu7aUI1JTwG/jzy/j7wu3EnEizUq9dEY9ZFGwnHv4Ugj9ZEY4Wji9/poI+G493CCabX1UfbWNiQs3/QUr66Q4Y/fAUjcDsJPRtN48w7gcNlQqx1GwO8j6BPn3S8EWgz7CPiFoN9HwOe+x01vPS3olxbrC/iFYFxZL3ZC7SZ0EfEDvwS+AmwDVorIUlX9KK7YN4FKVT1ZRK4DHgCuPRYBG2M6xu8TsjMCHO/H3UZjjc3JvSHWSEPUfbUajrjj9e60SExpiMYtE9O48jEiUW2eVx89vHw40siBQ9EW64vEWm6vK3cy7XF2FnFJ3k3+fr9w10UjubK0qOu3mUKZCcAnqroRQESeBq4E4hP6lcBcd/g54N9FRNSrjmKMMZ5zWq0+crpRX/6qSrRRicaUSGMj0ZgSjTUSaXTfY0rUnR6JOTuASMwt11S+1bSIuw5nesv1JSvbPzd0TD5fKjVdBFTEjW8Dzk5WRlWjIlINFAJ7uyJIY4zpCiJC0C8E/ZBF+t2MdlzvsRaRb4lIuYiU79mz53hu2hhj0l4qLfTtwJC48WJ3WqIy20QkAPTFOTnagqo+CjwKICJ7RGRLZ4IG+mOt/3hWHy1ZfRxmddFSOtTH0GQzUknoK4ERIjIMJ3FfB3y9VZmlwHTgL8DVwJ/bO36uqgNS2HZCIlKerD/g3sjqoyWrj8OsLlpK9/poN6G7x8RvB17DuWxxoaquE5H7gHJVXQr8FnhCRD4B9uMkfWOMMcdRSqefVfUV4JVW034YNxwGpnZtaMYYYzqip3Y8/ajXAXQzVh8tWX0cZnXRUlrXh2fPFDXGGNO1emoL3RhjTCs9LqGLyGQR2SAin4jI97yOxysiMkRE3hSRj0RknYh8x+uYugMR8YvIGhH5g9exeE1E8kXkORFZLyIfi8i5XsfkFRGZ7f6ffCgii0Wk430w9wA9KqHH9StzCTAauF5ERnsblWeiwJ2qOho4B7itF9dFvO8AH3sdRDfxC+BVVR0FnEEvrRcRKQJmAWWqOgbnar20vBKvRyV04vqVUdUGoKlfmV5HVT9T1dXucA3OP2vX9/bTg4hIMTAFWOB1LF4Tkb7AF3AuKUZVG1S1ytOgvBUAstwbH7OBHR7Hc0z0tISeqF+ZXp3EAESkBBgH/M3jULz2c2AO0LEnRqSnYcAe4DH3ENQCEUntoa9pRlW3Az8FtgKfAdWq+idvozo2elpCN62ISC7wPPAPqnrA63i8IiKXAbtVdZXXsXQTAeBM4GFVHQccBHrlOScRKcD5JT8MGAzkiMgN3kZ1bPS0hJ5KvzK9hogEcZL5U6r6gtfxeOx84AoR2YxzKO5LIvKktyF5ahuwTVWbfrU9h5Pge6MLgU2qukdVI8ALwHkex3RM9LSE3tyvjIhk4JzYWOpxTJ4Q53EovwU+VtWfeR2P11T1+6parKolOH8Xf1bVtGyFpUJVdwIVIjLSnfRlWj7DoDfZCpwjItnu/82XSdMTxN2n5/kUJOtXxuOwvHI+8A3gAxF5z512j9tNgzEAdwBPuY2fjcBMj+PxhKr+TUSeA1bjXB22hjS9Y9TuFDXGmDTR0w65GGOMScISujHGpAlL6MYYkyYsoRtjTJqwhG6MMWnCEroxxqQJS+jGGJMmLKEbY0ya+P+Oy9mKfSQJhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(t_loss, label=\"Training Loss\")\n",
    "plt.title(\"Loss at the end of each epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! \n",
    "If your accuracy is under 90%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.831838\n",
      "\n",
      "Accuracy for 0: 53.265%\n",
      "Accuracy for 1: 95.859%\n",
      "Accuracy for 2: 25.097%\n",
      "Accuracy for 3: 24.653%\n",
      "Accuracy for 4: 91.039%\n",
      "Accuracy for 5: 80.493%\n",
      "Accuracy for 6: 90.084%\n",
      "Accuracy for 7: 96.790%\n",
      "Accuracy for 8: 1.745%\n",
      "Accuracy for 9: 94.450%\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "test_accuracy = []\n",
    "\n",
    "model.eval() \n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = pred.eq(target)\n",
    "    for i in range(len(target)):\n",
    "        label = target[i].item()\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "for i in range(10):\n",
    "    accurate = class_correct[i]/class_total[i]\n",
    "    test_accuracy.append(accurate)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Accuracy for {i}: {test_accuracy[i] * 100:.3f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Epoch 1/12 is starting\n",
      "Batch 100/12000: train_loss = 2.29729\n",
      "Batch 200/12000: train_loss = 2.28436\n",
      "Batch 300/12000: train_loss = 2.27218\n",
      "Batch 400/12000: train_loss = 2.25783\n",
      "Batch 500/12000: train_loss = 2.24045\n",
      "Batch 600/12000: train_loss = 2.21996\n",
      "Batch 700/12000: train_loss = 2.20035\n",
      "Batch 800/12000: train_loss = 2.17583\n",
      "Batch 900/12000: train_loss = 2.14805\n",
      "Batch 1000/12000: train_loss = 2.11555\n",
      "Batch 1100/12000: train_loss = 2.07662\n",
      "Batch 1200/12000: train_loss = 2.03953\n",
      "Batch 1300/12000: train_loss = 1.99653\n",
      "Batch 1400/12000: train_loss = 1.95834\n",
      "Batch 1500/12000: train_loss = 1.92765\n",
      "Batch 1600/12000: train_loss = 1.89063\n",
      "Batch 1700/12000: train_loss = 1.85307\n",
      "Batch 1800/12000: train_loss = 1.81647\n",
      "Batch 1900/12000: train_loss = 1.77926\n",
      "Batch 2000/12000: train_loss = 1.74193\n",
      "Batch 2100/12000: train_loss = 1.70416\n",
      "Batch 2200/12000: train_loss = 1.66901\n",
      "Batch 2300/12000: train_loss = 1.63457\n",
      "Batch 2400/12000: train_loss = 1.60789\n",
      "Batch 2500/12000: train_loss = 1.57841\n",
      "Batch 2600/12000: train_loss = 1.55255\n",
      "Batch 2700/12000: train_loss = 1.52640\n",
      "Batch 2800/12000: train_loss = 1.50138\n",
      "Batch 2900/12000: train_loss = 1.47870\n",
      "Batch 3000/12000: train_loss = 1.45976\n",
      "Batch 3100/12000: train_loss = 1.43648\n",
      "Batch 3200/12000: train_loss = 1.41629\n",
      "Batch 3300/12000: train_loss = 1.39509\n",
      "Batch 3400/12000: train_loss = 1.37646\n",
      "Batch 3500/12000: train_loss = 1.35724\n",
      "Batch 3600/12000: train_loss = 1.34133\n",
      "Batch 3700/12000: train_loss = 1.32241\n",
      "Batch 3800/12000: train_loss = 1.30333\n",
      "Batch 3900/12000: train_loss = 1.28610\n",
      "Batch 4000/12000: train_loss = 1.26779\n",
      "Batch 4100/12000: train_loss = 1.25213\n",
      "Batch 4200/12000: train_loss = 1.23739\n",
      "Batch 4300/12000: train_loss = 1.22145\n",
      "Batch 4400/12000: train_loss = 1.20528\n",
      "Batch 4500/12000: train_loss = 1.19185\n",
      "Batch 4600/12000: train_loss = 1.17855\n",
      "Batch 4700/12000: train_loss = 1.16429\n",
      "Batch 4800/12000: train_loss = 1.15189\n",
      "Batch 4900/12000: train_loss = 1.13955\n",
      "Batch 5000/12000: train_loss = 1.12897\n",
      "Batch 5100/12000: train_loss = 1.11601\n",
      "Batch 5200/12000: train_loss = 1.10417\n",
      "Batch 5300/12000: train_loss = 1.09329\n",
      "Batch 5400/12000: train_loss = 1.08390\n",
      "Batch 5500/12000: train_loss = 1.07347\n",
      "Batch 5600/12000: train_loss = 1.06314\n",
      "Batch 5700/12000: train_loss = 1.05217\n",
      "Batch 5800/12000: train_loss = 1.04249\n",
      "Batch 5900/12000: train_loss = 1.03353\n",
      "Batch 6000/12000: train_loss = 1.02620\n",
      "Batch 6100/12000: train_loss = 1.01826\n",
      "Batch 6200/12000: train_loss = 1.01103\n",
      "Batch 6300/12000: train_loss = 1.00432\n",
      "Batch 6400/12000: train_loss = 0.99704\n",
      "Batch 6500/12000: train_loss = 0.99055\n",
      "Batch 6600/12000: train_loss = 0.98255\n",
      "Batch 6700/12000: train_loss = 0.97515\n",
      "Batch 6800/12000: train_loss = 0.96615\n",
      "Batch 6900/12000: train_loss = 0.95806\n",
      "Batch 7000/12000: train_loss = 0.95158\n",
      "Batch 7100/12000: train_loss = 0.94398\n",
      "Batch 7200/12000: train_loss = 0.93657\n",
      "Batch 7300/12000: train_loss = 0.92954\n",
      "Batch 7400/12000: train_loss = 0.92176\n",
      "Batch 7500/12000: train_loss = 0.91728\n",
      "Batch 7600/12000: train_loss = 0.91131\n",
      "Batch 7700/12000: train_loss = 0.90466\n",
      "Batch 7800/12000: train_loss = 0.89856\n",
      "Batch 7900/12000: train_loss = 0.89302\n",
      "Batch 8000/12000: train_loss = 0.88787\n",
      "Batch 8100/12000: train_loss = 0.88176\n",
      "Batch 8200/12000: train_loss = 0.87547\n",
      "Batch 8300/12000: train_loss = 0.87083\n",
      "Batch 8400/12000: train_loss = 0.86548\n",
      "Batch 8500/12000: train_loss = 0.86121\n",
      "Batch 8600/12000: train_loss = 0.85633\n",
      "Batch 8700/12000: train_loss = 0.85054\n",
      "Batch 8800/12000: train_loss = 0.84554\n",
      "Batch 8900/12000: train_loss = 0.84106\n",
      "Batch 9000/12000: train_loss = 0.83584\n",
      "Batch 9100/12000: train_loss = 0.83146\n",
      "Batch 9200/12000: train_loss = 0.82727\n",
      "Batch 9300/12000: train_loss = 0.82335\n",
      "Batch 9400/12000: train_loss = 0.81816\n",
      "Batch 9500/12000: train_loss = 0.81387\n",
      "Batch 9600/12000: train_loss = 0.80966\n",
      "Batch 9700/12000: train_loss = 0.80461\n",
      "Batch 9800/12000: train_loss = 0.80006\n",
      "Batch 9900/12000: train_loss = 0.79657\n",
      "Batch 10000/12000: train_loss = 0.79369\n",
      "Batch 10100/12000: train_loss = 0.79062\n",
      "Batch 10200/12000: train_loss = 0.78655\n",
      "Batch 10300/12000: train_loss = 0.78235\n",
      "Batch 10400/12000: train_loss = 0.77805\n",
      "Batch 10500/12000: train_loss = 0.77485\n",
      "Batch 10600/12000: train_loss = 0.77191\n",
      "Batch 10700/12000: train_loss = 0.76787\n",
      "Batch 10800/12000: train_loss = 0.76435\n",
      "Batch 10900/12000: train_loss = 0.76057\n",
      "Batch 11000/12000: train_loss = 0.75725\n",
      "Batch 11100/12000: train_loss = 0.75329\n",
      "Batch 11200/12000: train_loss = 0.74926\n",
      "Batch 11300/12000: train_loss = 0.74578\n",
      "Batch 11400/12000: train_loss = 0.74177\n",
      "Batch 11500/12000: train_loss = 0.73806\n",
      "Batch 11600/12000: train_loss = 0.73475\n",
      "Batch 11700/12000: train_loss = 0.73051\n",
      "Batch 11800/12000: train_loss = 0.72615\n",
      "Batch 11900/12000: train_loss = 0.72214\n",
      "Batch 12000/12000: train_loss = 0.71847\n",
      "Epoch 1/12 completed. Average training loss: 0.71847\n",
      "Epoch 2/12 is starting\n",
      "Batch 100/12000: train_loss = 0.33115\n",
      "Batch 200/12000: train_loss = 0.37649\n",
      "Batch 300/12000: train_loss = 0.39447\n",
      "Batch 400/12000: train_loss = 0.37083\n",
      "Batch 500/12000: train_loss = 0.35754\n",
      "Batch 600/12000: train_loss = 0.34868\n",
      "Batch 700/12000: train_loss = 0.34328\n",
      "Batch 800/12000: train_loss = 0.34064\n",
      "Batch 900/12000: train_loss = 0.33924\n",
      "Batch 1000/12000: train_loss = 0.33808\n",
      "Batch 1100/12000: train_loss = 0.33893\n",
      "Batch 1200/12000: train_loss = 0.33751\n",
      "Batch 1300/12000: train_loss = 0.33311\n",
      "Batch 1400/12000: train_loss = 0.33230\n",
      "Batch 1500/12000: train_loss = 0.33555\n",
      "Batch 1600/12000: train_loss = 0.33747\n",
      "Batch 1700/12000: train_loss = 0.33997\n",
      "Batch 1800/12000: train_loss = 0.34655\n",
      "Batch 1900/12000: train_loss = 0.34694\n",
      "Batch 2000/12000: train_loss = 0.34419\n",
      "Batch 2100/12000: train_loss = 0.34159\n",
      "Batch 2200/12000: train_loss = 0.33811\n",
      "Batch 2300/12000: train_loss = 0.33515\n",
      "Batch 2400/12000: train_loss = 0.33927\n",
      "Batch 2500/12000: train_loss = 0.33911\n",
      "Batch 2600/12000: train_loss = 0.34286\n",
      "Batch 2700/12000: train_loss = 0.34455\n",
      "Batch 2800/12000: train_loss = 0.34546\n",
      "Batch 2900/12000: train_loss = 0.34795\n",
      "Batch 3000/12000: train_loss = 0.35156\n",
      "Batch 3100/12000: train_loss = 0.35064\n",
      "Batch 3200/12000: train_loss = 0.35135\n",
      "Batch 3300/12000: train_loss = 0.35007\n",
      "Batch 3400/12000: train_loss = 0.35105\n",
      "Batch 3500/12000: train_loss = 0.35089\n",
      "Batch 3600/12000: train_loss = 0.35180\n",
      "Batch 3700/12000: train_loss = 0.34948\n",
      "Batch 3800/12000: train_loss = 0.34776\n",
      "Batch 3900/12000: train_loss = 0.34665\n",
      "Batch 4000/12000: train_loss = 0.34515\n",
      "Batch 4100/12000: train_loss = 0.34417\n",
      "Batch 4200/12000: train_loss = 0.34595\n",
      "Batch 4300/12000: train_loss = 0.34449\n",
      "Batch 4400/12000: train_loss = 0.34271\n",
      "Batch 4500/12000: train_loss = 0.34208\n",
      "Batch 4600/12000: train_loss = 0.34161\n",
      "Batch 4700/12000: train_loss = 0.34050\n",
      "Batch 4800/12000: train_loss = 0.34037\n",
      "Batch 4900/12000: train_loss = 0.33984\n",
      "Batch 5000/12000: train_loss = 0.34085\n",
      "Batch 5100/12000: train_loss = 0.33937\n",
      "Batch 5200/12000: train_loss = 0.33831\n",
      "Batch 5300/12000: train_loss = 0.33793\n",
      "Batch 5400/12000: train_loss = 0.33914\n",
      "Batch 5500/12000: train_loss = 0.33864\n",
      "Batch 5600/12000: train_loss = 0.33804\n",
      "Batch 5700/12000: train_loss = 0.33696\n",
      "Batch 5800/12000: train_loss = 0.33673\n",
      "Batch 5900/12000: train_loss = 0.33692\n",
      "Batch 6000/12000: train_loss = 0.33780\n",
      "Batch 6100/12000: train_loss = 0.33763\n",
      "Batch 6200/12000: train_loss = 0.33820\n",
      "Batch 6300/12000: train_loss = 0.33972\n",
      "Batch 6400/12000: train_loss = 0.34014\n",
      "Batch 6500/12000: train_loss = 0.34100\n",
      "Batch 6600/12000: train_loss = 0.34057\n",
      "Batch 6700/12000: train_loss = 0.34019\n",
      "Batch 6800/12000: train_loss = 0.33829\n",
      "Batch 6900/12000: train_loss = 0.33718\n",
      "Batch 7000/12000: train_loss = 0.33765\n",
      "Batch 7100/12000: train_loss = 0.33691\n",
      "Batch 7200/12000: train_loss = 0.33571\n",
      "Batch 7300/12000: train_loss = 0.33522\n",
      "Batch 7400/12000: train_loss = 0.33390\n",
      "Batch 7500/12000: train_loss = 0.33622\n",
      "Batch 7600/12000: train_loss = 0.33615\n",
      "Batch 7700/12000: train_loss = 0.33518\n",
      "Batch 7800/12000: train_loss = 0.33499\n",
      "Batch 7900/12000: train_loss = 0.33539\n",
      "Batch 8000/12000: train_loss = 0.33532\n",
      "Batch 8100/12000: train_loss = 0.33457\n",
      "Batch 8200/12000: train_loss = 0.33362\n",
      "Batch 8300/12000: train_loss = 0.33402\n",
      "Batch 8400/12000: train_loss = 0.33364\n",
      "Batch 8500/12000: train_loss = 0.33424\n",
      "Batch 8600/12000: train_loss = 0.33453\n",
      "Batch 8700/12000: train_loss = 0.33352\n",
      "Batch 8800/12000: train_loss = 0.33317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8900/12000: train_loss = 0.33310\n",
      "Batch 9000/12000: train_loss = 0.33240\n",
      "Batch 9100/12000: train_loss = 0.33236\n",
      "Batch 9200/12000: train_loss = 0.33266\n",
      "Batch 9300/12000: train_loss = 0.33319\n",
      "Batch 9400/12000: train_loss = 0.33214\n",
      "Batch 9500/12000: train_loss = 0.33197\n",
      "Batch 9600/12000: train_loss = 0.33184\n",
      "Batch 9700/12000: train_loss = 0.33083\n",
      "Batch 9800/12000: train_loss = 0.33016\n",
      "Batch 9900/12000: train_loss = 0.33040\n",
      "Batch 10000/12000: train_loss = 0.33115\n",
      "Batch 10100/12000: train_loss = 0.33179\n",
      "Batch 10200/12000: train_loss = 0.33134\n",
      "Batch 10300/12000: train_loss = 0.33062\n",
      "Batch 10400/12000: train_loss = 0.32990\n",
      "Batch 10500/12000: train_loss = 0.33007\n",
      "Batch 10600/12000: train_loss = 0.33070\n",
      "Batch 10700/12000: train_loss = 0.32996\n",
      "Batch 10800/12000: train_loss = 0.32988\n",
      "Batch 10900/12000: train_loss = 0.32937\n",
      "Batch 11000/12000: train_loss = 0.32925\n",
      "Batch 11100/12000: train_loss = 0.32853\n",
      "Batch 11200/12000: train_loss = 0.32773\n",
      "Batch 11300/12000: train_loss = 0.32731\n",
      "Batch 11400/12000: train_loss = 0.32633\n",
      "Batch 11500/12000: train_loss = 0.32545\n",
      "Batch 11600/12000: train_loss = 0.32504\n",
      "Batch 11700/12000: train_loss = 0.32374\n",
      "Batch 11800/12000: train_loss = 0.32235\n",
      "Batch 11900/12000: train_loss = 0.32121\n",
      "Batch 12000/12000: train_loss = 0.32053\n",
      "Epoch 2/12 completed. Average training loss: 0.32053\n",
      "Epoch 3/12 is starting\n",
      "Batch 100/12000: train_loss = 0.26111\n",
      "Batch 200/12000: train_loss = 0.30386\n",
      "Batch 300/12000: train_loss = 0.32400\n",
      "Batch 400/12000: train_loss = 0.29973\n",
      "Batch 500/12000: train_loss = 0.28562\n",
      "Batch 600/12000: train_loss = 0.27770\n",
      "Batch 700/12000: train_loss = 0.27105\n",
      "Batch 800/12000: train_loss = 0.26842\n",
      "Batch 900/12000: train_loss = 0.26816\n",
      "Batch 1000/12000: train_loss = 0.26743\n",
      "Batch 1100/12000: train_loss = 0.26916\n",
      "Batch 1200/12000: train_loss = 0.26872\n",
      "Batch 1300/12000: train_loss = 0.26479\n",
      "Batch 1400/12000: train_loss = 0.26465\n",
      "Batch 1500/12000: train_loss = 0.26727\n",
      "Batch 1600/12000: train_loss = 0.26900\n",
      "Batch 1700/12000: train_loss = 0.27174\n",
      "Batch 1800/12000: train_loss = 0.27875\n",
      "Batch 1900/12000: train_loss = 0.27972\n",
      "Batch 2000/12000: train_loss = 0.27727\n",
      "Batch 2100/12000: train_loss = 0.27554\n",
      "Batch 2200/12000: train_loss = 0.27264\n",
      "Batch 2300/12000: train_loss = 0.26989\n",
      "Batch 2400/12000: train_loss = 0.27390\n",
      "Batch 2500/12000: train_loss = 0.27415\n",
      "Batch 2600/12000: train_loss = 0.27853\n",
      "Batch 2700/12000: train_loss = 0.28013\n",
      "Batch 2800/12000: train_loss = 0.28115\n",
      "Batch 2900/12000: train_loss = 0.28345\n",
      "Batch 3000/12000: train_loss = 0.28679\n",
      "Batch 3100/12000: train_loss = 0.28644\n",
      "Batch 3200/12000: train_loss = 0.28708\n",
      "Batch 3300/12000: train_loss = 0.28587\n",
      "Batch 3400/12000: train_loss = 0.28669\n",
      "Batch 3500/12000: train_loss = 0.28657\n",
      "Batch 3600/12000: train_loss = 0.28731\n",
      "Batch 3700/12000: train_loss = 0.28520\n",
      "Batch 3800/12000: train_loss = 0.28382\n",
      "Batch 3900/12000: train_loss = 0.28272\n",
      "Batch 4000/12000: train_loss = 0.28163\n",
      "Batch 4100/12000: train_loss = 0.28070\n",
      "Batch 4200/12000: train_loss = 0.28308\n",
      "Batch 4300/12000: train_loss = 0.28219\n",
      "Batch 4400/12000: train_loss = 0.28101\n",
      "Batch 4500/12000: train_loss = 0.28051\n",
      "Batch 4600/12000: train_loss = 0.28033\n",
      "Batch 4700/12000: train_loss = 0.27956\n",
      "Batch 4800/12000: train_loss = 0.27966\n",
      "Batch 4900/12000: train_loss = 0.27912\n",
      "Batch 5000/12000: train_loss = 0.28021\n",
      "Batch 5100/12000: train_loss = 0.27899\n",
      "Batch 5200/12000: train_loss = 0.27830\n",
      "Batch 5300/12000: train_loss = 0.27802\n",
      "Batch 5400/12000: train_loss = 0.27933\n",
      "Batch 5500/12000: train_loss = 0.27914\n",
      "Batch 5600/12000: train_loss = 0.27872\n",
      "Batch 5700/12000: train_loss = 0.27791\n",
      "Batch 5800/12000: train_loss = 0.27789\n",
      "Batch 5900/12000: train_loss = 0.27810\n",
      "Batch 6000/12000: train_loss = 0.27893\n",
      "Batch 6100/12000: train_loss = 0.27871\n",
      "Batch 6200/12000: train_loss = 0.27912\n",
      "Batch 6300/12000: train_loss = 0.28079\n",
      "Batch 6400/12000: train_loss = 0.28124\n",
      "Batch 6500/12000: train_loss = 0.28206\n",
      "Batch 6600/12000: train_loss = 0.28181\n",
      "Batch 6700/12000: train_loss = 0.28157\n",
      "Batch 6800/12000: train_loss = 0.27997\n",
      "Batch 6900/12000: train_loss = 0.27911\n",
      "Batch 7000/12000: train_loss = 0.27963\n",
      "Batch 7100/12000: train_loss = 0.27925\n",
      "Batch 7200/12000: train_loss = 0.27817\n",
      "Batch 7300/12000: train_loss = 0.27794\n",
      "Batch 7400/12000: train_loss = 0.27690\n",
      "Batch 7500/12000: train_loss = 0.27940\n",
      "Batch 7600/12000: train_loss = 0.27950\n",
      "Batch 7700/12000: train_loss = 0.27865\n",
      "Batch 7800/12000: train_loss = 0.27860\n",
      "Batch 7900/12000: train_loss = 0.27915\n",
      "Batch 8000/12000: train_loss = 0.27904\n",
      "Batch 8100/12000: train_loss = 0.27847\n",
      "Batch 8200/12000: train_loss = 0.27776\n",
      "Batch 8300/12000: train_loss = 0.27832\n",
      "Batch 8400/12000: train_loss = 0.27802\n",
      "Batch 8500/12000: train_loss = 0.27871\n",
      "Batch 8600/12000: train_loss = 0.27920\n",
      "Batch 8700/12000: train_loss = 0.27842\n",
      "Batch 8800/12000: train_loss = 0.27820\n",
      "Batch 8900/12000: train_loss = 0.27818\n",
      "Batch 9000/12000: train_loss = 0.27767\n",
      "Batch 9100/12000: train_loss = 0.27770\n",
      "Batch 9200/12000: train_loss = 0.27816\n",
      "Batch 9300/12000: train_loss = 0.27893\n",
      "Batch 9400/12000: train_loss = 0.27803\n",
      "Batch 9500/12000: train_loss = 0.27802\n",
      "Batch 9600/12000: train_loss = 0.27801\n",
      "Batch 9700/12000: train_loss = 0.27717\n",
      "Batch 9800/12000: train_loss = 0.27667\n",
      "Batch 9900/12000: train_loss = 0.27696\n",
      "Batch 10000/12000: train_loss = 0.27776\n",
      "Batch 10100/12000: train_loss = 0.27847\n",
      "Batch 10200/12000: train_loss = 0.27817\n",
      "Batch 10300/12000: train_loss = 0.27750\n",
      "Batch 10400/12000: train_loss = 0.27696\n",
      "Batch 10500/12000: train_loss = 0.27717\n",
      "Batch 10600/12000: train_loss = 0.27793\n",
      "Batch 10700/12000: train_loss = 0.27729\n",
      "Batch 10800/12000: train_loss = 0.27741\n",
      "Batch 10900/12000: train_loss = 0.27699\n",
      "Batch 11000/12000: train_loss = 0.27695\n",
      "Batch 11100/12000: train_loss = 0.27640\n",
      "Batch 11200/12000: train_loss = 0.27577\n",
      "Batch 11300/12000: train_loss = 0.27549\n",
      "Batch 11400/12000: train_loss = 0.27464\n",
      "Batch 11500/12000: train_loss = 0.27385\n",
      "Batch 11600/12000: train_loss = 0.27354\n",
      "Batch 11700/12000: train_loss = 0.27244\n",
      "Batch 11800/12000: train_loss = 0.27130\n",
      "Batch 11900/12000: train_loss = 0.27034\n",
      "Batch 12000/12000: train_loss = 0.26991\n",
      "Epoch 3/12 completed. Average training loss: 0.26991\n",
      "Epoch 4/12 is starting\n",
      "Batch 100/12000: train_loss = 0.22510\n",
      "Batch 200/12000: train_loss = 0.26349\n",
      "Batch 300/12000: train_loss = 0.28507\n",
      "Batch 400/12000: train_loss = 0.26107\n",
      "Batch 500/12000: train_loss = 0.24695\n",
      "Batch 600/12000: train_loss = 0.23980\n",
      "Batch 700/12000: train_loss = 0.23303\n",
      "Batch 800/12000: train_loss = 0.22979\n",
      "Batch 900/12000: train_loss = 0.23023\n",
      "Batch 1000/12000: train_loss = 0.22955\n",
      "Batch 1100/12000: train_loss = 0.23126\n",
      "Batch 1200/12000: train_loss = 0.23123\n",
      "Batch 1300/12000: train_loss = 0.22755\n",
      "Batch 1400/12000: train_loss = 0.22783\n",
      "Batch 1500/12000: train_loss = 0.23027\n",
      "Batch 1600/12000: train_loss = 0.23155\n",
      "Batch 1700/12000: train_loss = 0.23414\n",
      "Batch 1800/12000: train_loss = 0.24090\n",
      "Batch 1900/12000: train_loss = 0.24186\n",
      "Batch 2000/12000: train_loss = 0.23949\n",
      "Batch 2100/12000: train_loss = 0.23828\n",
      "Batch 2200/12000: train_loss = 0.23592\n",
      "Batch 2300/12000: train_loss = 0.23327\n",
      "Batch 2400/12000: train_loss = 0.23685\n",
      "Batch 2500/12000: train_loss = 0.23701\n",
      "Batch 2600/12000: train_loss = 0.24151\n",
      "Batch 2700/12000: train_loss = 0.24280\n",
      "Batch 2800/12000: train_loss = 0.24373\n",
      "Batch 2900/12000: train_loss = 0.24571\n",
      "Batch 3000/12000: train_loss = 0.24867\n",
      "Batch 3100/12000: train_loss = 0.24860\n",
      "Batch 3200/12000: train_loss = 0.24911\n",
      "Batch 3300/12000: train_loss = 0.24793\n",
      "Batch 3400/12000: train_loss = 0.24846\n",
      "Batch 3500/12000: train_loss = 0.24829\n",
      "Batch 3600/12000: train_loss = 0.24888\n",
      "Batch 3700/12000: train_loss = 0.24702\n",
      "Batch 3800/12000: train_loss = 0.24576\n",
      "Batch 3900/12000: train_loss = 0.24473\n",
      "Batch 4000/12000: train_loss = 0.24390\n",
      "Batch 4100/12000: train_loss = 0.24295\n",
      "Batch 4200/12000: train_loss = 0.24549\n",
      "Batch 4300/12000: train_loss = 0.24491\n",
      "Batch 4400/12000: train_loss = 0.24410\n",
      "Batch 4500/12000: train_loss = 0.24372\n",
      "Batch 4600/12000: train_loss = 0.24367\n",
      "Batch 4700/12000: train_loss = 0.24307\n",
      "Batch 4800/12000: train_loss = 0.24324\n",
      "Batch 4900/12000: train_loss = 0.24262\n",
      "Batch 5000/12000: train_loss = 0.24360\n",
      "Batch 5100/12000: train_loss = 0.24248\n",
      "Batch 5200/12000: train_loss = 0.24205\n",
      "Batch 5300/12000: train_loss = 0.24177\n",
      "Batch 5400/12000: train_loss = 0.24304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/12000: train_loss = 0.24298\n",
      "Batch 5600/12000: train_loss = 0.24264\n",
      "Batch 5700/12000: train_loss = 0.24198\n",
      "Batch 5800/12000: train_loss = 0.24196\n",
      "Batch 5900/12000: train_loss = 0.24210\n",
      "Batch 6000/12000: train_loss = 0.24285\n",
      "Batch 6100/12000: train_loss = 0.24257\n",
      "Batch 6200/12000: train_loss = 0.24279\n",
      "Batch 6300/12000: train_loss = 0.24435\n",
      "Batch 6400/12000: train_loss = 0.24479\n",
      "Batch 6500/12000: train_loss = 0.24551\n",
      "Batch 6600/12000: train_loss = 0.24531\n",
      "Batch 6700/12000: train_loss = 0.24511\n",
      "Batch 6800/12000: train_loss = 0.24373\n",
      "Batch 6900/12000: train_loss = 0.24301\n",
      "Batch 7000/12000: train_loss = 0.24348\n",
      "Batch 7100/12000: train_loss = 0.24328\n",
      "Batch 7200/12000: train_loss = 0.24229\n",
      "Batch 7300/12000: train_loss = 0.24216\n",
      "Batch 7400/12000: train_loss = 0.24128\n",
      "Batch 7500/12000: train_loss = 0.24364\n",
      "Batch 7600/12000: train_loss = 0.24379\n",
      "Batch 7700/12000: train_loss = 0.24299\n",
      "Batch 7800/12000: train_loss = 0.24293\n",
      "Batch 7900/12000: train_loss = 0.24346\n",
      "Batch 8000/12000: train_loss = 0.24331\n",
      "Batch 8100/12000: train_loss = 0.24282\n",
      "Batch 8200/12000: train_loss = 0.24223\n",
      "Batch 8300/12000: train_loss = 0.24285\n",
      "Batch 8400/12000: train_loss = 0.24256\n",
      "Batch 8500/12000: train_loss = 0.24320\n",
      "Batch 8600/12000: train_loss = 0.24374\n",
      "Batch 8700/12000: train_loss = 0.24309\n",
      "Batch 8800/12000: train_loss = 0.24292\n",
      "Batch 8900/12000: train_loss = 0.24289\n",
      "Batch 9000/12000: train_loss = 0.24247\n",
      "Batch 9100/12000: train_loss = 0.24248\n",
      "Batch 9200/12000: train_loss = 0.24298\n",
      "Batch 9300/12000: train_loss = 0.24385\n",
      "Batch 9400/12000: train_loss = 0.24305\n",
      "Batch 9500/12000: train_loss = 0.24309\n",
      "Batch 9600/12000: train_loss = 0.24312\n",
      "Batch 9700/12000: train_loss = 0.24238\n",
      "Batch 9800/12000: train_loss = 0.24199\n",
      "Batch 9900/12000: train_loss = 0.24227\n",
      "Batch 10000/12000: train_loss = 0.24306\n",
      "Batch 10100/12000: train_loss = 0.24373\n",
      "Batch 10200/12000: train_loss = 0.24351\n",
      "Batch 10300/12000: train_loss = 0.24286\n",
      "Batch 10400/12000: train_loss = 0.24241\n",
      "Batch 10500/12000: train_loss = 0.24257\n",
      "Batch 10600/12000: train_loss = 0.24334\n",
      "Batch 10700/12000: train_loss = 0.24273\n",
      "Batch 10800/12000: train_loss = 0.24293\n",
      "Batch 10900/12000: train_loss = 0.24253\n",
      "Batch 11000/12000: train_loss = 0.24247\n",
      "Batch 11100/12000: train_loss = 0.24200\n",
      "Batch 11200/12000: train_loss = 0.24146\n",
      "Batch 11300/12000: train_loss = 0.24123\n",
      "Batch 11400/12000: train_loss = 0.24045\n",
      "Batch 11500/12000: train_loss = 0.23971\n",
      "Batch 11600/12000: train_loss = 0.23944\n",
      "Batch 11700/12000: train_loss = 0.23845\n",
      "Batch 11800/12000: train_loss = 0.23747\n",
      "Batch 11900/12000: train_loss = 0.23659\n",
      "Batch 12000/12000: train_loss = 0.23630\n",
      "Epoch 4/12 completed. Average training loss: 0.23630\n",
      "Epoch 5/12 is starting\n",
      "Batch 100/12000: train_loss = 0.19918\n",
      "Batch 200/12000: train_loss = 0.23203\n",
      "Batch 300/12000: train_loss = 0.25403\n",
      "Batch 400/12000: train_loss = 0.23148\n",
      "Batch 500/12000: train_loss = 0.21757\n",
      "Batch 600/12000: train_loss = 0.21086\n",
      "Batch 700/12000: train_loss = 0.20413\n",
      "Batch 800/12000: train_loss = 0.20038\n",
      "Batch 900/12000: train_loss = 0.20109\n",
      "Batch 1000/12000: train_loss = 0.20051\n",
      "Batch 1100/12000: train_loss = 0.20206\n",
      "Batch 1200/12000: train_loss = 0.20224\n",
      "Batch 1300/12000: train_loss = 0.19890\n",
      "Batch 1400/12000: train_loss = 0.19950\n",
      "Batch 1500/12000: train_loss = 0.20193\n",
      "Batch 1600/12000: train_loss = 0.20280\n",
      "Batch 1700/12000: train_loss = 0.20516\n",
      "Batch 1800/12000: train_loss = 0.21147\n",
      "Batch 1900/12000: train_loss = 0.21225\n",
      "Batch 2000/12000: train_loss = 0.21001\n",
      "Batch 2100/12000: train_loss = 0.20927\n",
      "Batch 2200/12000: train_loss = 0.20737\n",
      "Batch 2300/12000: train_loss = 0.20484\n",
      "Batch 2400/12000: train_loss = 0.20798\n",
      "Batch 2500/12000: train_loss = 0.20794\n",
      "Batch 2600/12000: train_loss = 0.21230\n",
      "Batch 2700/12000: train_loss = 0.21332\n",
      "Batch 2800/12000: train_loss = 0.21416\n",
      "Batch 2900/12000: train_loss = 0.21586\n",
      "Batch 3000/12000: train_loss = 0.21838\n",
      "Batch 3100/12000: train_loss = 0.21854\n",
      "Batch 3200/12000: train_loss = 0.21896\n",
      "Batch 3300/12000: train_loss = 0.21787\n",
      "Batch 3400/12000: train_loss = 0.21818\n",
      "Batch 3500/12000: train_loss = 0.21796\n",
      "Batch 3600/12000: train_loss = 0.21842\n",
      "Batch 3700/12000: train_loss = 0.21683\n",
      "Batch 3800/12000: train_loss = 0.21566\n",
      "Batch 3900/12000: train_loss = 0.21475\n",
      "Batch 4000/12000: train_loss = 0.21417\n",
      "Batch 4100/12000: train_loss = 0.21323\n",
      "Batch 4200/12000: train_loss = 0.21576\n",
      "Batch 4300/12000: train_loss = 0.21537\n",
      "Batch 4400/12000: train_loss = 0.21481\n",
      "Batch 4500/12000: train_loss = 0.21457\n",
      "Batch 4600/12000: train_loss = 0.21461\n",
      "Batch 4700/12000: train_loss = 0.21409\n",
      "Batch 4800/12000: train_loss = 0.21430\n",
      "Batch 4900/12000: train_loss = 0.21364\n",
      "Batch 5000/12000: train_loss = 0.21449\n",
      "Batch 5100/12000: train_loss = 0.21346\n",
      "Batch 5200/12000: train_loss = 0.21325\n",
      "Batch 5300/12000: train_loss = 0.21298\n",
      "Batch 5400/12000: train_loss = 0.21422\n",
      "Batch 5500/12000: train_loss = 0.21422\n",
      "Batch 5600/12000: train_loss = 0.21395\n",
      "Batch 5700/12000: train_loss = 0.21341\n",
      "Batch 5800/12000: train_loss = 0.21337\n",
      "Batch 5900/12000: train_loss = 0.21346\n",
      "Batch 6000/12000: train_loss = 0.21411\n",
      "Batch 6100/12000: train_loss = 0.21381\n",
      "Batch 6200/12000: train_loss = 0.21387\n",
      "Batch 6300/12000: train_loss = 0.21524\n",
      "Batch 6400/12000: train_loss = 0.21568\n",
      "Batch 6500/12000: train_loss = 0.21634\n",
      "Batch 6600/12000: train_loss = 0.21618\n",
      "Batch 6700/12000: train_loss = 0.21600\n",
      "Batch 6800/12000: train_loss = 0.21481\n",
      "Batch 6900/12000: train_loss = 0.21422\n",
      "Batch 7000/12000: train_loss = 0.21466\n",
      "Batch 7100/12000: train_loss = 0.21460\n",
      "Batch 7200/12000: train_loss = 0.21369\n",
      "Batch 7300/12000: train_loss = 0.21362\n",
      "Batch 7400/12000: train_loss = 0.21288\n",
      "Batch 7500/12000: train_loss = 0.21499\n",
      "Batch 7600/12000: train_loss = 0.21517\n",
      "Batch 7700/12000: train_loss = 0.21442\n",
      "Batch 7800/12000: train_loss = 0.21432\n",
      "Batch 7900/12000: train_loss = 0.21482\n",
      "Batch 8000/12000: train_loss = 0.21465\n",
      "Batch 8100/12000: train_loss = 0.21421\n",
      "Batch 8200/12000: train_loss = 0.21370\n",
      "Batch 8300/12000: train_loss = 0.21437\n",
      "Batch 8400/12000: train_loss = 0.21409\n",
      "Batch 8500/12000: train_loss = 0.21468\n",
      "Batch 8600/12000: train_loss = 0.21520\n",
      "Batch 8700/12000: train_loss = 0.21464\n",
      "Batch 8800/12000: train_loss = 0.21452\n",
      "Batch 8900/12000: train_loss = 0.21447\n",
      "Batch 9000/12000: train_loss = 0.21411\n",
      "Batch 9100/12000: train_loss = 0.21411\n",
      "Batch 9200/12000: train_loss = 0.21462\n",
      "Batch 9300/12000: train_loss = 0.21552\n",
      "Batch 9400/12000: train_loss = 0.21482\n",
      "Batch 9500/12000: train_loss = 0.21490\n",
      "Batch 9600/12000: train_loss = 0.21496\n",
      "Batch 9700/12000: train_loss = 0.21430\n",
      "Batch 9800/12000: train_loss = 0.21399\n",
      "Batch 9900/12000: train_loss = 0.21427\n",
      "Batch 10000/12000: train_loss = 0.21503\n",
      "Batch 10100/12000: train_loss = 0.21566\n",
      "Batch 10200/12000: train_loss = 0.21551\n",
      "Batch 10300/12000: train_loss = 0.21489\n",
      "Batch 10400/12000: train_loss = 0.21450\n",
      "Batch 10500/12000: train_loss = 0.21462\n",
      "Batch 10600/12000: train_loss = 0.21533\n",
      "Batch 10700/12000: train_loss = 0.21476\n",
      "Batch 10800/12000: train_loss = 0.21501\n",
      "Batch 10900/12000: train_loss = 0.21462\n",
      "Batch 11000/12000: train_loss = 0.21453\n",
      "Batch 11100/12000: train_loss = 0.21412\n",
      "Batch 11200/12000: train_loss = 0.21366\n",
      "Batch 11300/12000: train_loss = 0.21346\n",
      "Batch 11400/12000: train_loss = 0.21275\n",
      "Batch 11500/12000: train_loss = 0.21207\n",
      "Batch 11600/12000: train_loss = 0.21182\n",
      "Batch 11700/12000: train_loss = 0.21091\n",
      "Batch 11800/12000: train_loss = 0.21007\n",
      "Batch 11900/12000: train_loss = 0.20923\n",
      "Batch 12000/12000: train_loss = 0.20905\n",
      "Epoch 5/12 completed. Average training loss: 0.20905\n",
      "Epoch 6/12 is starting\n",
      "Batch 100/12000: train_loss = 0.17896\n",
      "Batch 200/12000: train_loss = 0.20602\n",
      "Batch 300/12000: train_loss = 0.22765\n",
      "Batch 400/12000: train_loss = 0.20710\n",
      "Batch 500/12000: train_loss = 0.19341\n",
      "Batch 600/12000: train_loss = 0.18690\n",
      "Batch 700/12000: train_loss = 0.18028\n",
      "Batch 800/12000: train_loss = 0.17619\n",
      "Batch 900/12000: train_loss = 0.17699\n",
      "Batch 1000/12000: train_loss = 0.17648\n",
      "Batch 1100/12000: train_loss = 0.17779\n",
      "Batch 1200/12000: train_loss = 0.17807\n",
      "Batch 1300/12000: train_loss = 0.17515\n",
      "Batch 1400/12000: train_loss = 0.17600\n",
      "Batch 1500/12000: train_loss = 0.17848\n",
      "Batch 1600/12000: train_loss = 0.17907\n",
      "Batch 1700/12000: train_loss = 0.18126\n",
      "Batch 1800/12000: train_loss = 0.18704\n",
      "Batch 1900/12000: train_loss = 0.18760\n",
      "Batch 2000/12000: train_loss = 0.18553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2100/12000: train_loss = 0.18525\n",
      "Batch 2200/12000: train_loss = 0.18373\n",
      "Batch 2300/12000: train_loss = 0.18130\n",
      "Batch 2400/12000: train_loss = 0.18403\n",
      "Batch 2500/12000: train_loss = 0.18378\n",
      "Batch 2600/12000: train_loss = 0.18785\n",
      "Batch 2700/12000: train_loss = 0.18867\n",
      "Batch 2800/12000: train_loss = 0.18943\n",
      "Batch 2900/12000: train_loss = 0.19092\n",
      "Batch 3000/12000: train_loss = 0.19299\n",
      "Batch 3100/12000: train_loss = 0.19334\n",
      "Batch 3200/12000: train_loss = 0.19370\n",
      "Batch 3300/12000: train_loss = 0.19273\n",
      "Batch 3400/12000: train_loss = 0.19289\n",
      "Batch 3500/12000: train_loss = 0.19264\n",
      "Batch 3600/12000: train_loss = 0.19298\n",
      "Batch 3700/12000: train_loss = 0.19165\n",
      "Batch 3800/12000: train_loss = 0.19053\n",
      "Batch 3900/12000: train_loss = 0.18976\n",
      "Batch 4000/12000: train_loss = 0.18940\n",
      "Batch 4100/12000: train_loss = 0.18851\n",
      "Batch 4200/12000: train_loss = 0.19097\n",
      "Batch 4300/12000: train_loss = 0.19072\n",
      "Batch 4400/12000: train_loss = 0.19034\n",
      "Batch 4500/12000: train_loss = 0.19020\n",
      "Batch 4600/12000: train_loss = 0.19028\n",
      "Batch 4700/12000: train_loss = 0.18979\n",
      "Batch 4800/12000: train_loss = 0.19003\n",
      "Batch 4900/12000: train_loss = 0.18936\n",
      "Batch 5000/12000: train_loss = 0.19010\n",
      "Batch 5100/12000: train_loss = 0.18915\n",
      "Batch 5200/12000: train_loss = 0.18912\n",
      "Batch 5300/12000: train_loss = 0.18890\n",
      "Batch 5400/12000: train_loss = 0.19009\n",
      "Batch 5500/12000: train_loss = 0.19012\n",
      "Batch 5600/12000: train_loss = 0.18991\n",
      "Batch 5700/12000: train_loss = 0.18948\n",
      "Batch 5800/12000: train_loss = 0.18942\n",
      "Batch 5900/12000: train_loss = 0.18948\n",
      "Batch 6000/12000: train_loss = 0.19004\n",
      "Batch 6100/12000: train_loss = 0.18975\n",
      "Batch 6200/12000: train_loss = 0.18969\n",
      "Batch 6300/12000: train_loss = 0.19087\n",
      "Batch 6400/12000: train_loss = 0.19133\n",
      "Batch 6500/12000: train_loss = 0.19194\n",
      "Batch 6600/12000: train_loss = 0.19183\n",
      "Batch 6700/12000: train_loss = 0.19167\n",
      "Batch 6800/12000: train_loss = 0.19064\n",
      "Batch 6900/12000: train_loss = 0.19015\n",
      "Batch 7000/12000: train_loss = 0.19056\n",
      "Batch 7100/12000: train_loss = 0.19063\n",
      "Batch 7200/12000: train_loss = 0.18979\n",
      "Batch 7300/12000: train_loss = 0.18977\n",
      "Batch 7400/12000: train_loss = 0.18914\n",
      "Batch 7500/12000: train_loss = 0.19104\n",
      "Batch 7600/12000: train_loss = 0.19122\n",
      "Batch 7700/12000: train_loss = 0.19052\n",
      "Batch 7800/12000: train_loss = 0.19039\n",
      "Batch 7900/12000: train_loss = 0.19085\n",
      "Batch 8000/12000: train_loss = 0.19068\n",
      "Batch 8100/12000: train_loss = 0.19029\n",
      "Batch 8200/12000: train_loss = 0.18984\n",
      "Batch 8300/12000: train_loss = 0.19055\n",
      "Batch 8400/12000: train_loss = 0.19030\n",
      "Batch 8500/12000: train_loss = 0.19084\n",
      "Batch 8600/12000: train_loss = 0.19134\n",
      "Batch 8700/12000: train_loss = 0.19083\n",
      "Batch 8800/12000: train_loss = 0.19074\n",
      "Batch 8900/12000: train_loss = 0.19069\n",
      "Batch 9000/12000: train_loss = 0.19038\n",
      "Batch 9100/12000: train_loss = 0.19036\n",
      "Batch 9200/12000: train_loss = 0.19086\n",
      "Batch 9300/12000: train_loss = 0.19176\n",
      "Batch 9400/12000: train_loss = 0.19114\n",
      "Batch 9500/12000: train_loss = 0.19127\n",
      "Batch 9600/12000: train_loss = 0.19136\n",
      "Batch 9700/12000: train_loss = 0.19076\n",
      "Batch 9800/12000: train_loss = 0.19053\n",
      "Batch 9900/12000: train_loss = 0.19080\n",
      "Batch 10000/12000: train_loss = 0.19153\n",
      "Batch 10100/12000: train_loss = 0.19211\n",
      "Batch 10200/12000: train_loss = 0.19202\n",
      "Batch 10300/12000: train_loss = 0.19145\n",
      "Batch 10400/12000: train_loss = 0.19112\n",
      "Batch 10500/12000: train_loss = 0.19120\n",
      "Batch 10600/12000: train_loss = 0.19184\n",
      "Batch 10700/12000: train_loss = 0.19131\n",
      "Batch 10800/12000: train_loss = 0.19159\n",
      "Batch 10900/12000: train_loss = 0.19120\n",
      "Batch 11000/12000: train_loss = 0.19109\n",
      "Batch 11100/12000: train_loss = 0.19073\n",
      "Batch 11200/12000: train_loss = 0.19034\n",
      "Batch 11300/12000: train_loss = 0.19018\n",
      "Batch 11400/12000: train_loss = 0.18954\n",
      "Batch 11500/12000: train_loss = 0.18891\n",
      "Batch 11600/12000: train_loss = 0.18867\n",
      "Batch 11700/12000: train_loss = 0.18785\n",
      "Batch 11800/12000: train_loss = 0.18710\n",
      "Batch 11900/12000: train_loss = 0.18631\n",
      "Batch 12000/12000: train_loss = 0.18622\n",
      "Epoch 6/12 completed. Average training loss: 0.18622\n",
      "Epoch 7/12 is starting\n",
      "Batch 100/12000: train_loss = 0.16210\n",
      "Batch 200/12000: train_loss = 0.18425\n",
      "Batch 300/12000: train_loss = 0.20540\n",
      "Batch 400/12000: train_loss = 0.18694\n",
      "Batch 500/12000: train_loss = 0.17344\n",
      "Batch 600/12000: train_loss = 0.16709\n",
      "Batch 700/12000: train_loss = 0.16055\n",
      "Batch 800/12000: train_loss = 0.15629\n",
      "Batch 900/12000: train_loss = 0.15705\n",
      "Batch 1000/12000: train_loss = 0.15662\n",
      "Batch 1100/12000: train_loss = 0.15770\n",
      "Batch 1200/12000: train_loss = 0.15805\n",
      "Batch 1300/12000: train_loss = 0.15556\n",
      "Batch 1400/12000: train_loss = 0.15658\n",
      "Batch 1500/12000: train_loss = 0.15908\n",
      "Batch 1600/12000: train_loss = 0.15950\n",
      "Batch 1700/12000: train_loss = 0.16157\n",
      "Batch 1800/12000: train_loss = 0.16680\n",
      "Batch 1900/12000: train_loss = 0.16717\n",
      "Batch 2000/12000: train_loss = 0.16531\n",
      "Batch 2100/12000: train_loss = 0.16541\n",
      "Batch 2200/12000: train_loss = 0.16416\n",
      "Batch 2300/12000: train_loss = 0.16185\n",
      "Batch 2400/12000: train_loss = 0.16418\n",
      "Batch 2500/12000: train_loss = 0.16375\n",
      "Batch 2600/12000: train_loss = 0.16744\n",
      "Batch 2700/12000: train_loss = 0.16809\n",
      "Batch 2800/12000: train_loss = 0.16878\n",
      "Batch 2900/12000: train_loss = 0.17010\n",
      "Batch 3000/12000: train_loss = 0.17175\n",
      "Batch 3100/12000: train_loss = 0.17224\n",
      "Batch 3200/12000: train_loss = 0.17257\n",
      "Batch 3300/12000: train_loss = 0.17173\n",
      "Batch 3400/12000: train_loss = 0.17180\n",
      "Batch 3500/12000: train_loss = 0.17153\n",
      "Batch 3600/12000: train_loss = 0.17180\n",
      "Batch 3700/12000: train_loss = 0.17067\n",
      "Batch 3800/12000: train_loss = 0.16958\n",
      "Batch 3900/12000: train_loss = 0.16894\n",
      "Batch 4000/12000: train_loss = 0.16874\n",
      "Batch 4100/12000: train_loss = 0.16793\n",
      "Batch 4200/12000: train_loss = 0.17031\n",
      "Batch 4300/12000: train_loss = 0.17014\n",
      "Batch 4400/12000: train_loss = 0.16988\n",
      "Batch 4500/12000: train_loss = 0.16982\n",
      "Batch 4600/12000: train_loss = 0.16992\n",
      "Batch 4700/12000: train_loss = 0.16945\n",
      "Batch 4800/12000: train_loss = 0.16970\n",
      "Batch 4900/12000: train_loss = 0.16905\n",
      "Batch 5000/12000: train_loss = 0.16966\n",
      "Batch 5100/12000: train_loss = 0.16879\n",
      "Batch 5200/12000: train_loss = 0.16890\n",
      "Batch 5300/12000: train_loss = 0.16872\n",
      "Batch 5400/12000: train_loss = 0.16989\n",
      "Batch 5500/12000: train_loss = 0.16992\n",
      "Batch 5600/12000: train_loss = 0.16976\n",
      "Batch 5700/12000: train_loss = 0.16942\n",
      "Batch 5800/12000: train_loss = 0.16935\n",
      "Batch 5900/12000: train_loss = 0.16938\n",
      "Batch 6000/12000: train_loss = 0.16985\n",
      "Batch 6100/12000: train_loss = 0.16958\n",
      "Batch 6200/12000: train_loss = 0.16943\n",
      "Batch 6300/12000: train_loss = 0.17044\n",
      "Batch 6400/12000: train_loss = 0.17091\n",
      "Batch 6500/12000: train_loss = 0.17149\n",
      "Batch 6600/12000: train_loss = 0.17144\n",
      "Batch 6700/12000: train_loss = 0.17129\n",
      "Batch 6800/12000: train_loss = 0.17039\n",
      "Batch 6900/12000: train_loss = 0.16997\n",
      "Batch 7000/12000: train_loss = 0.17038\n",
      "Batch 7100/12000: train_loss = 0.17055\n",
      "Batch 7200/12000: train_loss = 0.16977\n",
      "Batch 7300/12000: train_loss = 0.16978\n",
      "Batch 7400/12000: train_loss = 0.16924\n",
      "Batch 7500/12000: train_loss = 0.17096\n",
      "Batch 7600/12000: train_loss = 0.17114\n",
      "Batch 7700/12000: train_loss = 0.17048\n",
      "Batch 7800/12000: train_loss = 0.17034\n",
      "Batch 7900/12000: train_loss = 0.17076\n",
      "Batch 8000/12000: train_loss = 0.17059\n",
      "Batch 8100/12000: train_loss = 0.17023\n",
      "Batch 8200/12000: train_loss = 0.16984\n",
      "Batch 8300/12000: train_loss = 0.17057\n",
      "Batch 8400/12000: train_loss = 0.17034\n",
      "Batch 8500/12000: train_loss = 0.17083\n",
      "Batch 8600/12000: train_loss = 0.17131\n",
      "Batch 8700/12000: train_loss = 0.17082\n",
      "Batch 8800/12000: train_loss = 0.17076\n",
      "Batch 8900/12000: train_loss = 0.17071\n",
      "Batch 9000/12000: train_loss = 0.17045\n",
      "Batch 9100/12000: train_loss = 0.17042\n",
      "Batch 9200/12000: train_loss = 0.17091\n",
      "Batch 9300/12000: train_loss = 0.17178\n",
      "Batch 9400/12000: train_loss = 0.17125\n",
      "Batch 9500/12000: train_loss = 0.17143\n",
      "Batch 9600/12000: train_loss = 0.17155\n",
      "Batch 9700/12000: train_loss = 0.17101\n",
      "Batch 9800/12000: train_loss = 0.17084\n",
      "Batch 9900/12000: train_loss = 0.17108\n",
      "Batch 10000/12000: train_loss = 0.17179\n",
      "Batch 10100/12000: train_loss = 0.17232\n",
      "Batch 10200/12000: train_loss = 0.17228\n",
      "Batch 10300/12000: train_loss = 0.17175\n",
      "Batch 10400/12000: train_loss = 0.17147\n",
      "Batch 10500/12000: train_loss = 0.17152\n",
      "Batch 10600/12000: train_loss = 0.17209\n",
      "Batch 10700/12000: train_loss = 0.17161\n",
      "Batch 10800/12000: train_loss = 0.17190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10900/12000: train_loss = 0.17152\n",
      "Batch 11000/12000: train_loss = 0.17139\n",
      "Batch 11100/12000: train_loss = 0.17107\n",
      "Batch 11200/12000: train_loss = 0.17075\n",
      "Batch 11300/12000: train_loss = 0.17062\n",
      "Batch 11400/12000: train_loss = 0.17004\n",
      "Batch 11500/12000: train_loss = 0.16947\n",
      "Batch 11600/12000: train_loss = 0.16924\n",
      "Batch 11700/12000: train_loss = 0.16848\n",
      "Batch 11800/12000: train_loss = 0.16781\n",
      "Batch 11900/12000: train_loss = 0.16706\n",
      "Batch 12000/12000: train_loss = 0.16705\n",
      "Epoch 7/12 completed. Average training loss: 0.16705\n",
      "Epoch 8/12 is starting\n",
      "Batch 100/12000: train_loss = 0.14788\n",
      "Batch 200/12000: train_loss = 0.16601\n",
      "Batch 300/12000: train_loss = 0.18677\n",
      "Batch 400/12000: train_loss = 0.17016\n",
      "Batch 500/12000: train_loss = 0.15687\n",
      "Batch 600/12000: train_loss = 0.15074\n",
      "Batch 700/12000: train_loss = 0.14430\n",
      "Batch 800/12000: train_loss = 0.14000\n",
      "Batch 900/12000: train_loss = 0.14068\n",
      "Batch 1000/12000: train_loss = 0.14026\n",
      "Batch 1100/12000: train_loss = 0.14113\n",
      "Batch 1200/12000: train_loss = 0.14152\n",
      "Batch 1300/12000: train_loss = 0.13939\n",
      "Batch 1400/12000: train_loss = 0.14054\n",
      "Batch 1500/12000: train_loss = 0.14300\n",
      "Batch 1600/12000: train_loss = 0.14331\n",
      "Batch 1700/12000: train_loss = 0.14531\n",
      "Batch 1800/12000: train_loss = 0.14999\n",
      "Batch 1900/12000: train_loss = 0.15019\n",
      "Batch 2000/12000: train_loss = 0.14850\n",
      "Batch 2100/12000: train_loss = 0.14886\n",
      "Batch 2200/12000: train_loss = 0.14781\n",
      "Batch 2300/12000: train_loss = 0.14563\n",
      "Batch 2400/12000: train_loss = 0.14762\n",
      "Batch 2500/12000: train_loss = 0.14703\n",
      "Batch 2600/12000: train_loss = 0.15029\n",
      "Batch 2700/12000: train_loss = 0.15080\n",
      "Batch 2800/12000: train_loss = 0.15141\n",
      "Batch 2900/12000: train_loss = 0.15258\n",
      "Batch 3000/12000: train_loss = 0.15385\n",
      "Batch 3100/12000: train_loss = 0.15444\n",
      "Batch 3200/12000: train_loss = 0.15476\n",
      "Batch 3300/12000: train_loss = 0.15404\n",
      "Batch 3400/12000: train_loss = 0.15406\n",
      "Batch 3500/12000: train_loss = 0.15380\n",
      "Batch 3600/12000: train_loss = 0.15402\n",
      "Batch 3700/12000: train_loss = 0.15307\n",
      "Batch 3800/12000: train_loss = 0.15199\n",
      "Batch 3900/12000: train_loss = 0.15148\n",
      "Batch 4000/12000: train_loss = 0.15137\n",
      "Batch 4100/12000: train_loss = 0.15065\n",
      "Batch 4200/12000: train_loss = 0.15293\n",
      "Batch 4300/12000: train_loss = 0.15281\n",
      "Batch 4400/12000: train_loss = 0.15264\n",
      "Batch 4500/12000: train_loss = 0.15265\n",
      "Batch 4600/12000: train_loss = 0.15275\n",
      "Batch 4700/12000: train_loss = 0.15229\n",
      "Batch 4800/12000: train_loss = 0.15254\n",
      "Batch 4900/12000: train_loss = 0.15191\n",
      "Batch 5000/12000: train_loss = 0.15240\n",
      "Batch 5100/12000: train_loss = 0.15162\n",
      "Batch 5200/12000: train_loss = 0.15183\n",
      "Batch 5300/12000: train_loss = 0.15169\n",
      "Batch 5400/12000: train_loss = 0.15283\n",
      "Batch 5500/12000: train_loss = 0.15286\n",
      "Batch 5600/12000: train_loss = 0.15274\n",
      "Batch 5700/12000: train_loss = 0.15246\n",
      "Batch 5800/12000: train_loss = 0.15238\n",
      "Batch 5900/12000: train_loss = 0.15240\n",
      "Batch 6000/12000: train_loss = 0.15277\n",
      "Batch 6100/12000: train_loss = 0.15252\n",
      "Batch 6200/12000: train_loss = 0.15232\n",
      "Batch 6300/12000: train_loss = 0.15319\n",
      "Batch 6400/12000: train_loss = 0.15367\n",
      "Batch 6500/12000: train_loss = 0.15424\n",
      "Batch 6600/12000: train_loss = 0.15423\n",
      "Batch 6700/12000: train_loss = 0.15409\n",
      "Batch 6800/12000: train_loss = 0.15330\n",
      "Batch 6900/12000: train_loss = 0.15294\n",
      "Batch 7000/12000: train_loss = 0.15334\n",
      "Batch 7100/12000: train_loss = 0.15358\n",
      "Batch 7200/12000: train_loss = 0.15286\n",
      "Batch 7300/12000: train_loss = 0.15289\n",
      "Batch 7400/12000: train_loss = 0.15241\n",
      "Batch 7500/12000: train_loss = 0.15398\n",
      "Batch 7600/12000: train_loss = 0.15415\n",
      "Batch 7700/12000: train_loss = 0.15353\n",
      "Batch 7800/12000: train_loss = 0.15339\n",
      "Batch 7900/12000: train_loss = 0.15378\n",
      "Batch 8000/12000: train_loss = 0.15361\n",
      "Batch 8100/12000: train_loss = 0.15328\n",
      "Batch 8200/12000: train_loss = 0.15292\n",
      "Batch 8300/12000: train_loss = 0.15366\n",
      "Batch 8400/12000: train_loss = 0.15346\n",
      "Batch 8500/12000: train_loss = 0.15390\n",
      "Batch 8600/12000: train_loss = 0.15435\n",
      "Batch 8700/12000: train_loss = 0.15388\n",
      "Batch 8800/12000: train_loss = 0.15384\n",
      "Batch 8900/12000: train_loss = 0.15380\n",
      "Batch 9000/12000: train_loss = 0.15359\n",
      "Batch 9100/12000: train_loss = 0.15355\n",
      "Batch 9200/12000: train_loss = 0.15404\n",
      "Batch 9300/12000: train_loss = 0.15488\n",
      "Batch 9400/12000: train_loss = 0.15442\n",
      "Batch 9500/12000: train_loss = 0.15464\n",
      "Batch 9600/12000: train_loss = 0.15479\n",
      "Batch 9700/12000: train_loss = 0.15431\n",
      "Batch 9800/12000: train_loss = 0.15418\n",
      "Batch 9900/12000: train_loss = 0.15440\n",
      "Batch 10000/12000: train_loss = 0.15509\n",
      "Batch 10100/12000: train_loss = 0.15557\n",
      "Batch 10200/12000: train_loss = 0.15556\n",
      "Batch 10300/12000: train_loss = 0.15509\n",
      "Batch 10400/12000: train_loss = 0.15485\n",
      "Batch 10500/12000: train_loss = 0.15488\n",
      "Batch 10600/12000: train_loss = 0.15538\n",
      "Batch 10700/12000: train_loss = 0.15494\n",
      "Batch 10800/12000: train_loss = 0.15524\n",
      "Batch 10900/12000: train_loss = 0.15486\n",
      "Batch 11000/12000: train_loss = 0.15472\n",
      "Batch 11100/12000: train_loss = 0.15445\n",
      "Batch 11200/12000: train_loss = 0.15417\n",
      "Batch 11300/12000: train_loss = 0.15408\n",
      "Batch 11400/12000: train_loss = 0.15354\n",
      "Batch 11500/12000: train_loss = 0.15303\n",
      "Batch 11600/12000: train_loss = 0.15280\n",
      "Batch 11700/12000: train_loss = 0.15211\n",
      "Batch 11800/12000: train_loss = 0.15151\n",
      "Batch 11900/12000: train_loss = 0.15079\n",
      "Batch 12000/12000: train_loss = 0.15085\n",
      "Epoch 8/12 completed. Average training loss: 0.15085\n",
      "Epoch 9/12 is starting\n",
      "Batch 100/12000: train_loss = 0.13558\n",
      "Batch 200/12000: train_loss = 0.15056\n",
      "Batch 300/12000: train_loss = 0.17107\n",
      "Batch 400/12000: train_loss = 0.15601\n",
      "Batch 500/12000: train_loss = 0.14295\n",
      "Batch 600/12000: train_loss = 0.13714\n",
      "Batch 700/12000: train_loss = 0.13076\n",
      "Batch 800/12000: train_loss = 0.12651\n",
      "Batch 900/12000: train_loss = 0.12710\n",
      "Batch 1000/12000: train_loss = 0.12667\n",
      "Batch 1100/12000: train_loss = 0.12733\n",
      "Batch 1200/12000: train_loss = 0.12777\n",
      "Batch 1300/12000: train_loss = 0.12589\n",
      "Batch 1400/12000: train_loss = 0.12713\n",
      "Batch 1500/12000: train_loss = 0.12956\n",
      "Batch 1600/12000: train_loss = 0.12977\n",
      "Batch 1700/12000: train_loss = 0.13171\n",
      "Batch 1800/12000: train_loss = 0.13585\n",
      "Batch 1900/12000: train_loss = 0.13591\n",
      "Batch 2000/12000: train_loss = 0.13433\n",
      "Batch 2100/12000: train_loss = 0.13490\n",
      "Batch 2200/12000: train_loss = 0.13396\n",
      "Batch 2300/12000: train_loss = 0.13191\n",
      "Batch 2400/12000: train_loss = 0.13364\n",
      "Batch 2500/12000: train_loss = 0.13294\n",
      "Batch 2600/12000: train_loss = 0.13578\n",
      "Batch 2700/12000: train_loss = 0.13616\n",
      "Batch 2800/12000: train_loss = 0.13669\n",
      "Batch 2900/12000: train_loss = 0.13771\n",
      "Batch 3000/12000: train_loss = 0.13866\n",
      "Batch 3100/12000: train_loss = 0.13932\n",
      "Batch 3200/12000: train_loss = 0.13965\n",
      "Batch 3300/12000: train_loss = 0.13903\n",
      "Batch 3400/12000: train_loss = 0.13904\n",
      "Batch 3500/12000: train_loss = 0.13879\n",
      "Batch 3600/12000: train_loss = 0.13898\n",
      "Batch 3700/12000: train_loss = 0.13818\n",
      "Batch 3800/12000: train_loss = 0.13711\n",
      "Batch 3900/12000: train_loss = 0.13671\n",
      "Batch 4000/12000: train_loss = 0.13667\n",
      "Batch 4100/12000: train_loss = 0.13604\n",
      "Batch 4200/12000: train_loss = 0.13820\n",
      "Batch 4300/12000: train_loss = 0.13810\n",
      "Batch 4400/12000: train_loss = 0.13800\n",
      "Batch 4500/12000: train_loss = 0.13806\n",
      "Batch 4600/12000: train_loss = 0.13814\n",
      "Batch 4700/12000: train_loss = 0.13771\n",
      "Batch 4800/12000: train_loss = 0.13796\n",
      "Batch 4900/12000: train_loss = 0.13736\n",
      "Batch 5000/12000: train_loss = 0.13773\n",
      "Batch 5100/12000: train_loss = 0.13703\n",
      "Batch 5200/12000: train_loss = 0.13731\n",
      "Batch 5300/12000: train_loss = 0.13722\n",
      "Batch 5400/12000: train_loss = 0.13833\n",
      "Batch 5500/12000: train_loss = 0.13835\n",
      "Batch 5600/12000: train_loss = 0.13827\n",
      "Batch 5700/12000: train_loss = 0.13802\n",
      "Batch 5800/12000: train_loss = 0.13793\n",
      "Batch 5900/12000: train_loss = 0.13794\n",
      "Batch 6000/12000: train_loss = 0.13822\n",
      "Batch 6100/12000: train_loss = 0.13801\n",
      "Batch 6200/12000: train_loss = 0.13777\n",
      "Batch 6300/12000: train_loss = 0.13853\n",
      "Batch 6400/12000: train_loss = 0.13901\n",
      "Batch 6500/12000: train_loss = 0.13956\n",
      "Batch 6600/12000: train_loss = 0.13959\n",
      "Batch 6700/12000: train_loss = 0.13946\n",
      "Batch 6800/12000: train_loss = 0.13877\n",
      "Batch 6900/12000: train_loss = 0.13846\n",
      "Batch 7000/12000: train_loss = 0.13884\n",
      "Batch 7100/12000: train_loss = 0.13913\n",
      "Batch 7200/12000: train_loss = 0.13846\n",
      "Batch 7300/12000: train_loss = 0.13849\n",
      "Batch 7400/12000: train_loss = 0.13808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/12000: train_loss = 0.13953\n",
      "Batch 7600/12000: train_loss = 0.13968\n",
      "Batch 7700/12000: train_loss = 0.13911\n",
      "Batch 7800/12000: train_loss = 0.13896\n",
      "Batch 7900/12000: train_loss = 0.13933\n",
      "Batch 8000/12000: train_loss = 0.13918\n",
      "Batch 8100/12000: train_loss = 0.13886\n",
      "Batch 8200/12000: train_loss = 0.13854\n",
      "Batch 8300/12000: train_loss = 0.13926\n",
      "Batch 8400/12000: train_loss = 0.13909\n",
      "Batch 8500/12000: train_loss = 0.13947\n",
      "Batch 8600/12000: train_loss = 0.13991\n",
      "Batch 8700/12000: train_loss = 0.13945\n",
      "Batch 8800/12000: train_loss = 0.13942\n",
      "Batch 8900/12000: train_loss = 0.13939\n",
      "Batch 9000/12000: train_loss = 0.13921\n",
      "Batch 9100/12000: train_loss = 0.13917\n",
      "Batch 9200/12000: train_loss = 0.13966\n",
      "Batch 9300/12000: train_loss = 0.14047\n",
      "Batch 9400/12000: train_loss = 0.14006\n",
      "Batch 9500/12000: train_loss = 0.14032\n",
      "Batch 9600/12000: train_loss = 0.14050\n",
      "Batch 9700/12000: train_loss = 0.14006\n",
      "Batch 9800/12000: train_loss = 0.13996\n",
      "Batch 9900/12000: train_loss = 0.14017\n",
      "Batch 10000/12000: train_loss = 0.14084\n",
      "Batch 10100/12000: train_loss = 0.14127\n",
      "Batch 10200/12000: train_loss = 0.14128\n",
      "Batch 10300/12000: train_loss = 0.14086\n",
      "Batch 10400/12000: train_loss = 0.14066\n",
      "Batch 10500/12000: train_loss = 0.14067\n",
      "Batch 10600/12000: train_loss = 0.14111\n",
      "Batch 10700/12000: train_loss = 0.14070\n",
      "Batch 10800/12000: train_loss = 0.14099\n",
      "Batch 10900/12000: train_loss = 0.14063\n",
      "Batch 11000/12000: train_loss = 0.14048\n",
      "Batch 11100/12000: train_loss = 0.14025\n",
      "Batch 11200/12000: train_loss = 0.14002\n",
      "Batch 11300/12000: train_loss = 0.13994\n",
      "Batch 11400/12000: train_loss = 0.13945\n",
      "Batch 11500/12000: train_loss = 0.13899\n",
      "Batch 11600/12000: train_loss = 0.13877\n",
      "Batch 11700/12000: train_loss = 0.13814\n",
      "Batch 11800/12000: train_loss = 0.13759\n",
      "Batch 11900/12000: train_loss = 0.13691\n",
      "Batch 12000/12000: train_loss = 0.13703\n",
      "Epoch 9/12 completed. Average training loss: 0.13703\n",
      "Epoch 10/12 is starting\n",
      "Batch 100/12000: train_loss = 0.12453\n",
      "Batch 200/12000: train_loss = 0.13714\n",
      "Batch 300/12000: train_loss = 0.15738\n",
      "Batch 400/12000: train_loss = 0.14371\n",
      "Batch 500/12000: train_loss = 0.13092\n",
      "Batch 600/12000: train_loss = 0.12553\n",
      "Batch 700/12000: train_loss = 0.11921\n",
      "Batch 800/12000: train_loss = 0.11513\n",
      "Batch 900/12000: train_loss = 0.11563\n",
      "Batch 1000/12000: train_loss = 0.11514\n",
      "Batch 1100/12000: train_loss = 0.11564\n",
      "Batch 1200/12000: train_loss = 0.11610\n",
      "Batch 1300/12000: train_loss = 0.11442\n",
      "Batch 1400/12000: train_loss = 0.11575\n",
      "Batch 1500/12000: train_loss = 0.11813\n",
      "Batch 1600/12000: train_loss = 0.11827\n",
      "Batch 1700/12000: train_loss = 0.12017\n",
      "Batch 1800/12000: train_loss = 0.12382\n",
      "Batch 1900/12000: train_loss = 0.12377\n",
      "Batch 2000/12000: train_loss = 0.12228\n",
      "Batch 2100/12000: train_loss = 0.12301\n",
      "Batch 2200/12000: train_loss = 0.12214\n",
      "Batch 2300/12000: train_loss = 0.12021\n",
      "Batch 2400/12000: train_loss = 0.12174\n",
      "Batch 2500/12000: train_loss = 0.12098\n",
      "Batch 2600/12000: train_loss = 0.12343\n",
      "Batch 2700/12000: train_loss = 0.12370\n",
      "Batch 2800/12000: train_loss = 0.12416\n",
      "Batch 2900/12000: train_loss = 0.12503\n",
      "Batch 3000/12000: train_loss = 0.12575\n",
      "Batch 3100/12000: train_loss = 0.12642\n",
      "Batch 3200/12000: train_loss = 0.12676\n",
      "Batch 3300/12000: train_loss = 0.12623\n",
      "Batch 3400/12000: train_loss = 0.12622\n",
      "Batch 3500/12000: train_loss = 0.12600\n",
      "Batch 3600/12000: train_loss = 0.12619\n",
      "Batch 3700/12000: train_loss = 0.12551\n",
      "Batch 3800/12000: train_loss = 0.12447\n",
      "Batch 3900/12000: train_loss = 0.12417\n",
      "Batch 4000/12000: train_loss = 0.12416\n",
      "Batch 4100/12000: train_loss = 0.12360\n",
      "Batch 4200/12000: train_loss = 0.12565\n",
      "Batch 4300/12000: train_loss = 0.12557\n",
      "Batch 4400/12000: train_loss = 0.12551\n",
      "Batch 4500/12000: train_loss = 0.12561\n",
      "Batch 4600/12000: train_loss = 0.12567\n",
      "Batch 4700/12000: train_loss = 0.12526\n",
      "Batch 4800/12000: train_loss = 0.12552\n",
      "Batch 4900/12000: train_loss = 0.12494\n",
      "Batch 5000/12000: train_loss = 0.12522\n",
      "Batch 5100/12000: train_loss = 0.12460\n",
      "Batch 5200/12000: train_loss = 0.12492\n",
      "Batch 5300/12000: train_loss = 0.12487\n",
      "Batch 5400/12000: train_loss = 0.12596\n",
      "Batch 5500/12000: train_loss = 0.12597\n",
      "Batch 5600/12000: train_loss = 0.12592\n",
      "Batch 5700/12000: train_loss = 0.12568\n",
      "Batch 5800/12000: train_loss = 0.12559\n",
      "Batch 5900/12000: train_loss = 0.12559\n",
      "Batch 6000/12000: train_loss = 0.12580\n",
      "Batch 6100/12000: train_loss = 0.12561\n",
      "Batch 6200/12000: train_loss = 0.12536\n",
      "Batch 6300/12000: train_loss = 0.12602\n",
      "Batch 6400/12000: train_loss = 0.12649\n",
      "Batch 6500/12000: train_loss = 0.12702\n",
      "Batch 6600/12000: train_loss = 0.12708\n",
      "Batch 6700/12000: train_loss = 0.12697\n",
      "Batch 6800/12000: train_loss = 0.12634\n",
      "Batch 6900/12000: train_loss = 0.12608\n",
      "Batch 7000/12000: train_loss = 0.12644\n",
      "Batch 7100/12000: train_loss = 0.12675\n",
      "Batch 7200/12000: train_loss = 0.12614\n",
      "Batch 7300/12000: train_loss = 0.12617\n",
      "Batch 7400/12000: train_loss = 0.12580\n",
      "Batch 7500/12000: train_loss = 0.12715\n",
      "Batch 7600/12000: train_loss = 0.12730\n",
      "Batch 7700/12000: train_loss = 0.12676\n",
      "Batch 7800/12000: train_loss = 0.12662\n",
      "Batch 7900/12000: train_loss = 0.12697\n",
      "Batch 8000/12000: train_loss = 0.12682\n",
      "Batch 8100/12000: train_loss = 0.12652\n",
      "Batch 8200/12000: train_loss = 0.12622\n",
      "Batch 8300/12000: train_loss = 0.12691\n",
      "Batch 8400/12000: train_loss = 0.12677\n",
      "Batch 8500/12000: train_loss = 0.12711\n",
      "Batch 8600/12000: train_loss = 0.12753\n",
      "Batch 8700/12000: train_loss = 0.12707\n",
      "Batch 8800/12000: train_loss = 0.12706\n",
      "Batch 8900/12000: train_loss = 0.12704\n",
      "Batch 9000/12000: train_loss = 0.12689\n",
      "Batch 9100/12000: train_loss = 0.12684\n",
      "Batch 9200/12000: train_loss = 0.12732\n",
      "Batch 9300/12000: train_loss = 0.12810\n",
      "Batch 9400/12000: train_loss = 0.12774\n",
      "Batch 9500/12000: train_loss = 0.12802\n",
      "Batch 9600/12000: train_loss = 0.12823\n",
      "Batch 9700/12000: train_loss = 0.12783\n",
      "Batch 9800/12000: train_loss = 0.12776\n",
      "Batch 9900/12000: train_loss = 0.12795\n",
      "Batch 10000/12000: train_loss = 0.12861\n",
      "Batch 10100/12000: train_loss = 0.12898\n",
      "Batch 10200/12000: train_loss = 0.12901\n",
      "Batch 10300/12000: train_loss = 0.12863\n",
      "Batch 10400/12000: train_loss = 0.12847\n",
      "Batch 10500/12000: train_loss = 0.12847\n",
      "Batch 10600/12000: train_loss = 0.12886\n",
      "Batch 10700/12000: train_loss = 0.12848\n",
      "Batch 10800/12000: train_loss = 0.12875\n",
      "Batch 10900/12000: train_loss = 0.12840\n",
      "Batch 11000/12000: train_loss = 0.12826\n",
      "Batch 11100/12000: train_loss = 0.12806\n",
      "Batch 11200/12000: train_loss = 0.12786\n",
      "Batch 11300/12000: train_loss = 0.12781\n",
      "Batch 11400/12000: train_loss = 0.12735\n",
      "Batch 11500/12000: train_loss = 0.12693\n",
      "Batch 11600/12000: train_loss = 0.12672\n",
      "Batch 11700/12000: train_loss = 0.12615\n",
      "Batch 11800/12000: train_loss = 0.12565\n",
      "Batch 11900/12000: train_loss = 0.12500\n",
      "Batch 12000/12000: train_loss = 0.12518\n",
      "Epoch 10/12 completed. Average training loss: 0.12518\n",
      "Epoch 11/12 is starting\n",
      "Batch 100/12000: train_loss = 0.11469\n",
      "Batch 200/12000: train_loss = 0.12533\n",
      "Batch 300/12000: train_loss = 0.14536\n",
      "Batch 400/12000: train_loss = 0.13291\n",
      "Batch 500/12000: train_loss = 0.12040\n",
      "Batch 600/12000: train_loss = 0.11551\n",
      "Batch 700/12000: train_loss = 0.10928\n",
      "Batch 800/12000: train_loss = 0.10536\n",
      "Batch 900/12000: train_loss = 0.10576\n",
      "Batch 1000/12000: train_loss = 0.10520\n",
      "Batch 1100/12000: train_loss = 0.10553\n",
      "Batch 1200/12000: train_loss = 0.10601\n",
      "Batch 1300/12000: train_loss = 0.10452\n",
      "Batch 1400/12000: train_loss = 0.10593\n",
      "Batch 1500/12000: train_loss = 0.10824\n",
      "Batch 1600/12000: train_loss = 0.10835\n",
      "Batch 1700/12000: train_loss = 0.11022\n",
      "Batch 1800/12000: train_loss = 0.11345\n",
      "Batch 1900/12000: train_loss = 0.11331\n",
      "Batch 2000/12000: train_loss = 0.11188\n",
      "Batch 2100/12000: train_loss = 0.11276\n",
      "Batch 2200/12000: train_loss = 0.11192\n",
      "Batch 2300/12000: train_loss = 0.11010\n",
      "Batch 2400/12000: train_loss = 0.11146\n",
      "Batch 2500/12000: train_loss = 0.11067\n",
      "Batch 2600/12000: train_loss = 0.11276\n",
      "Batch 2700/12000: train_loss = 0.11293\n",
      "Batch 2800/12000: train_loss = 0.11333\n",
      "Batch 2900/12000: train_loss = 0.11408\n",
      "Batch 3000/12000: train_loss = 0.11462\n",
      "Batch 3100/12000: train_loss = 0.11527\n",
      "Batch 3200/12000: train_loss = 0.11563\n",
      "Batch 3300/12000: train_loss = 0.11518\n",
      "Batch 3400/12000: train_loss = 0.11517\n",
      "Batch 3500/12000: train_loss = 0.11498\n",
      "Batch 3600/12000: train_loss = 0.11517\n",
      "Batch 3700/12000: train_loss = 0.11459\n",
      "Batch 3800/12000: train_loss = 0.11359\n",
      "Batch 3900/12000: train_loss = 0.11337\n",
      "Batch 4000/12000: train_loss = 0.11337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4100/12000: train_loss = 0.11288\n",
      "Batch 4200/12000: train_loss = 0.11482\n",
      "Batch 4300/12000: train_loss = 0.11474\n",
      "Batch 4400/12000: train_loss = 0.11471\n",
      "Batch 4500/12000: train_loss = 0.11484\n",
      "Batch 4600/12000: train_loss = 0.11488\n",
      "Batch 4700/12000: train_loss = 0.11450\n",
      "Batch 4800/12000: train_loss = 0.11477\n",
      "Batch 4900/12000: train_loss = 0.11421\n",
      "Batch 5000/12000: train_loss = 0.11441\n",
      "Batch 5100/12000: train_loss = 0.11385\n",
      "Batch 5200/12000: train_loss = 0.11421\n",
      "Batch 5300/12000: train_loss = 0.11419\n",
      "Batch 5400/12000: train_loss = 0.11525\n",
      "Batch 5500/12000: train_loss = 0.11526\n",
      "Batch 5600/12000: train_loss = 0.11524\n",
      "Batch 5700/12000: train_loss = 0.11501\n",
      "Batch 5800/12000: train_loss = 0.11492\n",
      "Batch 5900/12000: train_loss = 0.11490\n",
      "Batch 6000/12000: train_loss = 0.11503\n",
      "Batch 6100/12000: train_loss = 0.11486\n",
      "Batch 6200/12000: train_loss = 0.11461\n",
      "Batch 6300/12000: train_loss = 0.11519\n",
      "Batch 6400/12000: train_loss = 0.11565\n",
      "Batch 6500/12000: train_loss = 0.11616\n",
      "Batch 6600/12000: train_loss = 0.11624\n",
      "Batch 6700/12000: train_loss = 0.11614\n",
      "Batch 6800/12000: train_loss = 0.11558\n",
      "Batch 6900/12000: train_loss = 0.11536\n",
      "Batch 7000/12000: train_loss = 0.11568\n",
      "Batch 7100/12000: train_loss = 0.11600\n",
      "Batch 7200/12000: train_loss = 0.11545\n",
      "Batch 7300/12000: train_loss = 0.11549\n",
      "Batch 7400/12000: train_loss = 0.11516\n",
      "Batch 7500/12000: train_loss = 0.11642\n",
      "Batch 7600/12000: train_loss = 0.11655\n",
      "Batch 7700/12000: train_loss = 0.11605\n",
      "Batch 7800/12000: train_loss = 0.11591\n",
      "Batch 7900/12000: train_loss = 0.11624\n",
      "Batch 8000/12000: train_loss = 0.11610\n",
      "Batch 8100/12000: train_loss = 0.11582\n",
      "Batch 8200/12000: train_loss = 0.11554\n",
      "Batch 8300/12000: train_loss = 0.11620\n",
      "Batch 8400/12000: train_loss = 0.11609\n",
      "Batch 8500/12000: train_loss = 0.11637\n",
      "Batch 8600/12000: train_loss = 0.11678\n",
      "Batch 8700/12000: train_loss = 0.11633\n",
      "Batch 8800/12000: train_loss = 0.11633\n",
      "Batch 8900/12000: train_loss = 0.11633\n",
      "Batch 9000/12000: train_loss = 0.11620\n",
      "Batch 9100/12000: train_loss = 0.11614\n",
      "Batch 9200/12000: train_loss = 0.11661\n",
      "Batch 9300/12000: train_loss = 0.11735\n",
      "Batch 9400/12000: train_loss = 0.11704\n",
      "Batch 9500/12000: train_loss = 0.11734\n",
      "Batch 9600/12000: train_loss = 0.11757\n",
      "Batch 9700/12000: train_loss = 0.11721\n",
      "Batch 9800/12000: train_loss = 0.11716\n",
      "Batch 9900/12000: train_loss = 0.11733\n",
      "Batch 10000/12000: train_loss = 0.11797\n",
      "Batch 10100/12000: train_loss = 0.11829\n",
      "Batch 10200/12000: train_loss = 0.11833\n",
      "Batch 10300/12000: train_loss = 0.11800\n",
      "Batch 10400/12000: train_loss = 0.11787\n",
      "Batch 10500/12000: train_loss = 0.11786\n",
      "Batch 10600/12000: train_loss = 0.11820\n",
      "Batch 10700/12000: train_loss = 0.11785\n",
      "Batch 10800/12000: train_loss = 0.11810\n",
      "Batch 10900/12000: train_loss = 0.11777\n",
      "Batch 11000/12000: train_loss = 0.11764\n",
      "Batch 11100/12000: train_loss = 0.11747\n",
      "Batch 11200/12000: train_loss = 0.11729\n",
      "Batch 11300/12000: train_loss = 0.11725\n",
      "Batch 11400/12000: train_loss = 0.11683\n",
      "Batch 11500/12000: train_loss = 0.11645\n",
      "Batch 11600/12000: train_loss = 0.11624\n",
      "Batch 11700/12000: train_loss = 0.11572\n",
      "Batch 11800/12000: train_loss = 0.11526\n",
      "Batch 11900/12000: train_loss = 0.11466\n",
      "Batch 12000/12000: train_loss = 0.11488\n",
      "Epoch 11/12 completed. Average training loss: 0.11488\n",
      "Epoch 12/12 is starting\n",
      "Batch 100/12000: train_loss = 0.10590\n",
      "Batch 200/12000: train_loss = 0.11488\n",
      "Batch 300/12000: train_loss = 0.13463\n",
      "Batch 400/12000: train_loss = 0.12326\n",
      "Batch 500/12000: train_loss = 0.11109\n",
      "Batch 600/12000: train_loss = 0.10672\n",
      "Batch 700/12000: train_loss = 0.10062\n",
      "Batch 800/12000: train_loss = 0.09690\n",
      "Batch 900/12000: train_loss = 0.09718\n",
      "Batch 1000/12000: train_loss = 0.09654\n",
      "Batch 1100/12000: train_loss = 0.09677\n",
      "Batch 1200/12000: train_loss = 0.09724\n",
      "Batch 1300/12000: train_loss = 0.09591\n",
      "Batch 1400/12000: train_loss = 0.09740\n",
      "Batch 1500/12000: train_loss = 0.09964\n",
      "Batch 1600/12000: train_loss = 0.09973\n",
      "Batch 1700/12000: train_loss = 0.10157\n",
      "Batch 1800/12000: train_loss = 0.10444\n",
      "Batch 1900/12000: train_loss = 0.10425\n",
      "Batch 2000/12000: train_loss = 0.10285\n",
      "Batch 2100/12000: train_loss = 0.10384\n",
      "Batch 2200/12000: train_loss = 0.10302\n",
      "Batch 2300/12000: train_loss = 0.10129\n",
      "Batch 2400/12000: train_loss = 0.10252\n",
      "Batch 2500/12000: train_loss = 0.10172\n",
      "Batch 2600/12000: train_loss = 0.10348\n",
      "Batch 2700/12000: train_loss = 0.10357\n",
      "Batch 2800/12000: train_loss = 0.10392\n",
      "Batch 2900/12000: train_loss = 0.10453\n",
      "Batch 3000/12000: train_loss = 0.10493\n",
      "Batch 3100/12000: train_loss = 0.10555\n",
      "Batch 3200/12000: train_loss = 0.10592\n",
      "Batch 3300/12000: train_loss = 0.10553\n",
      "Batch 3400/12000: train_loss = 0.10552\n",
      "Batch 3500/12000: train_loss = 0.10537\n",
      "Batch 3600/12000: train_loss = 0.10556\n",
      "Batch 3700/12000: train_loss = 0.10507\n",
      "Batch 3800/12000: train_loss = 0.10411\n",
      "Batch 3900/12000: train_loss = 0.10396\n",
      "Batch 4000/12000: train_loss = 0.10395\n",
      "Batch 4100/12000: train_loss = 0.10352\n",
      "Batch 4200/12000: train_loss = 0.10534\n",
      "Batch 4300/12000: train_loss = 0.10526\n",
      "Batch 4400/12000: train_loss = 0.10525\n",
      "Batch 4500/12000: train_loss = 0.10540\n",
      "Batch 4600/12000: train_loss = 0.10542\n",
      "Batch 4700/12000: train_loss = 0.10510\n",
      "Batch 4800/12000: train_loss = 0.10536\n",
      "Batch 4900/12000: train_loss = 0.10482\n",
      "Batch 5000/12000: train_loss = 0.10496\n",
      "Batch 5100/12000: train_loss = 0.10445\n",
      "Batch 5200/12000: train_loss = 0.10485\n",
      "Batch 5300/12000: train_loss = 0.10485\n",
      "Batch 5400/12000: train_loss = 0.10589\n",
      "Batch 5500/12000: train_loss = 0.10590\n",
      "Batch 5600/12000: train_loss = 0.10590\n",
      "Batch 5700/12000: train_loss = 0.10567\n",
      "Batch 5800/12000: train_loss = 0.10558\n",
      "Batch 5900/12000: train_loss = 0.10555\n",
      "Batch 6000/12000: train_loss = 0.10562\n",
      "Batch 6100/12000: train_loss = 0.10545\n",
      "Batch 6200/12000: train_loss = 0.10521\n",
      "Batch 6300/12000: train_loss = 0.10572\n",
      "Batch 6400/12000: train_loss = 0.10616\n",
      "Batch 6500/12000: train_loss = 0.10664\n",
      "Batch 6600/12000: train_loss = 0.10674\n",
      "Batch 6700/12000: train_loss = 0.10666\n",
      "Batch 6800/12000: train_loss = 0.10614\n",
      "Batch 6900/12000: train_loss = 0.10596\n",
      "Batch 7000/12000: train_loss = 0.10626\n",
      "Batch 7100/12000: train_loss = 0.10658\n",
      "Batch 7200/12000: train_loss = 0.10608\n",
      "Batch 7300/12000: train_loss = 0.10613\n",
      "Batch 7400/12000: train_loss = 0.10582\n",
      "Batch 7500/12000: train_loss = 0.10700\n",
      "Batch 7600/12000: train_loss = 0.10713\n",
      "Batch 7700/12000: train_loss = 0.10665\n",
      "Batch 7800/12000: train_loss = 0.10653\n",
      "Batch 7900/12000: train_loss = 0.10684\n",
      "Batch 8000/12000: train_loss = 0.10671\n",
      "Batch 8100/12000: train_loss = 0.10644\n",
      "Batch 8200/12000: train_loss = 0.10618\n",
      "Batch 8300/12000: train_loss = 0.10681\n",
      "Batch 8400/12000: train_loss = 0.10671\n",
      "Batch 8500/12000: train_loss = 0.10696\n",
      "Batch 8600/12000: train_loss = 0.10735\n",
      "Batch 8700/12000: train_loss = 0.10692\n",
      "Batch 8800/12000: train_loss = 0.10692\n",
      "Batch 8900/12000: train_loss = 0.10695\n",
      "Batch 9000/12000: train_loss = 0.10683\n",
      "Batch 9100/12000: train_loss = 0.10676\n",
      "Batch 9200/12000: train_loss = 0.10722\n",
      "Batch 9300/12000: train_loss = 0.10793\n",
      "Batch 9400/12000: train_loss = 0.10766\n",
      "Batch 9500/12000: train_loss = 0.10796\n",
      "Batch 9600/12000: train_loss = 0.10823\n",
      "Batch 9700/12000: train_loss = 0.10789\n",
      "Batch 9800/12000: train_loss = 0.10785\n",
      "Batch 9900/12000: train_loss = 0.10802\n",
      "Batch 10000/12000: train_loss = 0.10864\n",
      "Batch 10100/12000: train_loss = 0.10891\n",
      "Batch 10200/12000: train_loss = 0.10895\n",
      "Batch 10300/12000: train_loss = 0.10866\n",
      "Batch 10400/12000: train_loss = 0.10856\n",
      "Batch 10500/12000: train_loss = 0.10854\n",
      "Batch 10600/12000: train_loss = 0.10884\n",
      "Batch 10700/12000: train_loss = 0.10851\n",
      "Batch 10800/12000: train_loss = 0.10874\n",
      "Batch 10900/12000: train_loss = 0.10843\n",
      "Batch 11000/12000: train_loss = 0.10831\n",
      "Batch 11100/12000: train_loss = 0.10817\n",
      "Batch 11200/12000: train_loss = 0.10800\n",
      "Batch 11300/12000: train_loss = 0.10797\n",
      "Batch 11400/12000: train_loss = 0.10758\n",
      "Batch 11500/12000: train_loss = 0.10723\n",
      "Batch 11600/12000: train_loss = 0.10704\n",
      "Batch 11700/12000: train_loss = 0.10657\n",
      "Batch 11800/12000: train_loss = 0.10614\n",
      "Batch 11900/12000: train_loss = 0.10557\n",
      "Batch 12000/12000: train_loss = 0.10583\n",
      "Epoch 12/12 completed. Average training loss: 0.10583\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Create an instance of your neural network\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "print(f'Device: {device}')\n",
    "\n",
    "n_epochs = 12\n",
    "t_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs} is starting')\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for i, (data, target) in enumerate(train_dataloader, 1):\n",
    "        data = data.to(device)  # Move data to the same device as the model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Batch {i}/{len(train_dataloader)}: train_loss = {train_loss / (i * data.size(0)):.5f}')\n",
    "    \n",
    "    # Calculate the average training loss for the epoch\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    t_loss.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{n_epochs} completed. Average training loss: {train_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 11.618191\n",
      "\n",
      "Accuracy for 0: 9.286%\n",
      "Accuracy for 1: 24.846%\n",
      "Accuracy for 2: 2.326%\n",
      "Accuracy for 3: 0.000%\n",
      "Accuracy for 4: 100.000%\n",
      "Accuracy for 5: 45.516%\n",
      "Accuracy for 6: 12.839%\n",
      "Accuracy for 7: 59.533%\n",
      "Accuracy for 8: 0.000%\n",
      "Accuracy for 9: 0.000%\n"
     ]
    }
   ],
   "source": [
    "## TESTING MODEL ##\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "test_accuracy = []\n",
    "\n",
    "model.eval() \n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = pred.eq(target)\n",
    "    for i in range(len(target)):\n",
    "        label = target[i].item()\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "for i in range(10):\n",
    "    accurate = class_correct[i]/class_total[i]\n",
    "    test_accuracy.append(accurate)\n",
    "\n",
    "\n",
    "TypeError: state_dict() missing 1 required positional argument: 'self'\n",
    "    print(f'Accuracy for {i}: {test_accuracy[i] * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "model = NeuralNetwork()\n",
    "for i in range(10):\n",
    "    torch.save(model.state_dict(), 'Mnist-Classification-Project.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
